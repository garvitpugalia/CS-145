{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-layer Neural Network Workbook for CS145 Homework 3\n",
    "\n",
    "----\n",
    "<span style=\"color:red\">**PRINT YOUR NAME AND UID HERE!**</span>\n",
    "\n",
    "NAME: [PUGALIA, GARVIT RAJKUMAR] UID: [504628127]\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "Please follow the notebook linearly to implement a two layer neural network.\n",
    "\n",
    "Please print out the workbook entirely when completed.\n",
    "\n",
    "The goal of this workbook is to give you experience with training a two layer neural network.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and define relative error function, which is used to check results later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs145.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example\n",
    "\n",
    "Before loading CIFAR-10, there will be a toy example to test your implementation of the forward and backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.neural_net import TwoLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small net and some toy data to check your implementations.\n",
    "# Note that we set the random seed for repeatable experiments.\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "    np.random.seed(0)\n",
    "    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute forward pass scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your scores:\n",
      "[[-1.07260209  0.05083871 -0.87253915]\n",
      " [-2.02778743 -0.10832494 -1.52641362]\n",
      " [-0.74225908  0.15259725 -0.39578548]\n",
      " [-0.38172726  0.10835902 -0.17328274]\n",
      " [-0.64417314 -0.18886813 -0.41106892]]\n",
      "\n",
      "correct scores:\n",
      "[[-1.07260209  0.05083871 -0.87253915]\n",
      " [-2.02778743 -0.10832494 -1.52641362]\n",
      " [-0.74225908  0.15259725 -0.39578548]\n",
      " [-0.38172726  0.10835902 -0.17328274]\n",
      " [-0.64417314 -0.18886813 -0.41106892]]\n",
      "\n",
      "Difference between your scores and correct scores:\n",
      "3.381231204052648e-08\n"
     ]
    }
   ],
   "source": [
    "## Implement the forward pass of the neural network.\n",
    "\n",
    "# Note, there is a statement if y is None: return scores, which is why \n",
    "# the following call will calculate the scores.\n",
    "scores = net.loss(X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('correct scores:')\n",
    "correct_scores = np.asarray([\n",
    "    [-1.07260209,  0.05083871, -0.87253915],\n",
    "    [-2.02778743, -0.10832494, -1.52641362],\n",
    "    [-0.74225908,  0.15259725, -0.39578548],\n",
    "    [-0.38172726,  0.10835902, -0.17328274],\n",
    "    [-0.64417314, -0.18886813, -0.41106892]])\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass loss\n",
    "\n",
    "The total loss includes data loss (MSE) and regularization loss, which is,\n",
    "\n",
    "$$L = L_{data}+L_{reg} = \\frac{1}{2N}\\sum_{i=1}^{N}\\left(\\boldsymbol{y}_{\\text{pred}}-\\boldsymbol{y}_{\\text{target}}\\right)^2 + \\frac{\\lambda}{2} \\left(||W_1||^2 + ||W_2||^2 \\right)$$\n",
    "\n",
    "More specifically in multi-class situation, if the output of neural nets from one sample is $y_{\\text{pred}}=(0.1,0.1,0.8)$ and $y_{\\text{target}}=(0,0,1)$ from the given label, then the MSE error will be $Error=(0.1-0)^2+(0.1-0)^2+(0.8-1)^2=0.06$\n",
    "\n",
    "Implement data loss and regularization loss. In the MSE function, you also need to return the gradients which need to be passed backward. This is similar to batch gradient in linear regression. Test your implementation of loss functions. The Difference should be less than 1e-12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between your loss and correct loss:\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "loss, _ = net.loss(X, y, reg=0.05)\n",
    "correct_loss_MSE = 1.8973332763705641 # check this number\n",
    "\n",
    "# should be very small, we get < 1e-12\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(np.sum(np.abs(loss - correct_loss_MSE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass (You do not need to implemented this part)\n",
    "\n",
    "We have already implemented the backwards pass of the neural network for you.  Run the block of code to check your gradients with the gradient check utilities provided. The results should be automatically correct (tiny relative error).\n",
    "\n",
    "If there is a gradient error larger than 1e-8, the training for neural networks later will be negatively affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 max relative error: 6.774278173332322e-11\n",
      "b2 max relative error: 1.887502392114964e-11\n",
      "W1 max relative error: 1.7476665046687833e-09\n",
      "b1 max relative error: 7.382451041178829e-10\n"
     ]
    }
   ],
   "source": [
    "from cs145.gradient_check import eval_numerical_gradient\n",
    "\n",
    "# Use numeric gradient checking to check your implementation of the backward pass.\n",
    "# If your implementation is correct, the difference between the numeric and\n",
    "# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.05)\n",
    "\n",
    "# these should all be less than 1e-8 or so\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(X, y, reg=0.05)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print('{} max relative error: {}'.format(param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "\n",
    "Implement neural_net.train() to train the network via stochastic gradient descent, much like the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss:  0.02950555626206818\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = init_toy_model()\n",
    "stats = net.train(X, y, X, y,\n",
    "            learning_rate=1e-1, reg=5e-6,\n",
    "            num_iters=100, verbose=False)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify CIFAR-10\n",
    "\n",
    "Do classification on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3072)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3072)\n",
      "Test labels shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "from cs145.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = './cs145/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    # Reshape data to rows\n",
    "    X_train = X_train.reshape(num_training, -1)\n",
    "    X_val = X_val.reshape(num_validation, -1)\n",
    "    X_test = X_test.reshape(num_test, -1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SGD\n",
    "\n",
    "If your implementation is correct, you should see a validation accuracy of around 15-18%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 0.5000623457905099\n",
      "iteration 100 / 1000: loss 0.49982465294352774\n",
      "iteration 200 / 1000: loss 0.4995946718475304\n",
      "iteration 300 / 1000: loss 0.49933536166627984\n",
      "iteration 400 / 1000: loss 0.4989962372581251\n",
      "iteration 500 / 1000: loss 0.49847178744773624\n",
      "iteration 600 / 1000: loss 0.49758927830530253\n",
      "iteration 700 / 1000: loss 0.4966248113033766\n",
      "iteration 800 / 1000: loss 0.4958001901438695\n",
      "iteration 900 / 1000: loss 0.4939583435911163\n",
      "Validation accuracy:  0.172\n"
     ]
    }
   ],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=1000, batch_size=200,\n",
    "            learning_rate=1e-5, learning_rate_decay=0.95,\n",
    "            reg=0.1, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (net.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc)\n",
    "\n",
    "# Save this net as the variable subopt_net for later comparison.\n",
    "subopt_net = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09, 0.15, 0.225, 0.195, 0.225]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats['train_acc_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "The training accuracy isn't great. It seems even worse than simple KNN model, which is not as good as expected.\n",
    "\n",
    "(1) What are some of the reasons why this is the case?  Take the following cell to do some analyses and then report your answers in the cell following the one below.\n",
    "\n",
    "(2) How should you fix the problems you identified in (1)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the neural network\n",
    "\n",
    "Use the following part of the Jupyter notebook to optimize your hyperparameters on the validation set.  Store your nets as best_net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1: loss 0.5001850047685655\n",
      "iteration 0 / 1: loss 0.5000838774333805\n",
      "iteration 0 / 1: loss 0.5001798070681149\n",
      "iteration 0 / 1: loss 0.5001085518423148\n",
      "iteration 0 / 1: loss 0.49975972859470447\n",
      "iteration 0 / 1: loss 0.5001560056985098\n",
      "iteration 0 / 1: loss 0.4998931643257986\n",
      "iteration 0 / 1: loss 0.5000311807804234\n",
      "iteration 0 / 1: loss 0.5001970860442992\n",
      "iteration 0 / 1: loss 0.499866639834343\n",
      "iteration 0 / 1: loss 0.5000752066866219\n",
      "iteration 0 / 1: loss 0.5002084097395162\n",
      "iteration 0 / 1: loss 0.5002566421539645\n",
      "iteration 0 / 1: loss 0.5001585726411816\n",
      "iteration 0 / 1: loss 0.5002505934022862\n",
      "iteration 0 / 1: loss 0.4999968813989994\n",
      "iteration 0 / 1: loss 0.5001486667481302\n",
      "iteration 0 / 1: loss 0.4999860880686066\n",
      "iteration 0 / 1: loss 0.5000255797501172\n",
      "iteration 0 / 1: loss 0.49942770822315224\n",
      "iteration 0 / 1: loss 0.5002468974959654\n",
      "iteration 0 / 1: loss 0.5000156968732082\n",
      "iteration 0 / 1: loss 0.4995892947410939\n",
      "iteration 0 / 1: loss 0.4999836686617537\n",
      "iteration 0 / 1: loss 0.5002055171090194\n",
      "iteration 0 / 1: loss 0.49998771989011975\n",
      "iteration 0 / 1: loss 0.5000714246524471\n",
      "iteration 0 / 1: loss 0.4996922597599966\n",
      "iteration 0 / 1: loss 0.49975237635387976\n",
      "iteration 0 / 1: loss 0.4999903512018608\n",
      "iteration 0 / 1: loss 0.49976839942639\n",
      "iteration 0 / 1: loss 0.5000201166243485\n",
      "iteration 0 / 1: loss 0.5001435722081025\n",
      "iteration 0 / 1: loss 0.4998536153634988\n",
      "iteration 0 / 1: loss 0.499784261701778\n",
      "iteration 0 / 1: loss 0.4999359791048373\n",
      "iteration 0 / 1: loss 0.5000212423324474\n",
      "iteration 0 / 1: loss 0.5003233907941538\n",
      "iteration 0 / 1: loss 0.5001197596956205\n",
      "iteration 0 / 1: loss 0.49970259536036044\n",
      "iteration 0 / 1: loss 0.5000683099303093\n",
      "iteration 0 / 1: loss 0.5002898719992595\n",
      "iteration 0 / 1: loss 0.4997002397011091\n",
      "iteration 0 / 1: loss 0.4998249500532951\n",
      "iteration 0 / 1: loss 0.4999632803009254\n",
      "iteration 0 / 1: loss 0.500081491768276\n",
      "iteration 0 / 1: loss 0.5002584898845126\n",
      "iteration 0 / 1: loss 0.4999023275212307\n",
      "iteration 0 / 1: loss 0.49974547646329237\n",
      "iteration 0 / 1: loss 0.5002290899407767\n",
      "iteration 0 / 1: loss 0.5001231961545382\n",
      "iteration 0 / 1: loss 0.5002900988617833\n",
      "iteration 0 / 1: loss 0.5001670695260645\n",
      "iteration 0 / 1: loss 0.5000875303911061\n",
      "iteration 0 / 1: loss 0.5002144526350984\n",
      "iteration 0 / 1: loss 0.500211405263132\n",
      "iteration 0 / 1: loss 0.5000497549330734\n",
      "iteration 0 / 1: loss 0.4996882071833088\n",
      "iteration 0 / 1: loss 0.49998093323867265\n",
      "iteration 0 / 1: loss 0.49986165180074266\n",
      "iteration 0 / 1: loss 0.5002418194557824\n",
      "iteration 0 / 1: loss 0.49998583554341186\n",
      "iteration 0 / 1: loss 0.500002312266362\n",
      "iteration 0 / 1: loss 0.5000865824575615\n",
      "iteration 0 / 1: loss 0.5000002210015668\n",
      "iteration 0 / 1: loss 0.49988177567822895\n",
      "iteration 0 / 1: loss 0.5001322691140703\n",
      "iteration 0 / 1: loss 0.49997723446497844\n",
      "iteration 0 / 1: loss 0.5000216777623531\n",
      "iteration 0 / 1: loss 0.500282734933031\n",
      "iteration 0 / 1: loss 0.4998877947240014\n",
      "iteration 0 / 1: loss 0.49998135451736936\n",
      "iteration 0 / 1: loss 0.49984755570043155\n",
      "iteration 0 / 1: loss 0.5001825451995767\n",
      "iteration 0 / 1: loss 0.4999825685725515\n",
      "iteration 0 / 1: loss 0.5001829994412824\n",
      "iteration 0 / 1: loss 0.500008861878539\n",
      "iteration 0 / 1: loss 0.5003229163925171\n",
      "iteration 0 / 1: loss 0.5000594525174809\n",
      "iteration 0 / 1: loss 0.4998094588953424\n",
      "iteration 0 / 1: loss 0.5000808336402514\n",
      "iteration 0 / 1: loss 0.500083669090076\n",
      "iteration 0 / 1: loss 0.500015573767625\n",
      "iteration 0 / 1: loss 0.5002203780094092\n",
      "iteration 0 / 1: loss 0.4998219036106052\n",
      "iteration 0 / 1: loss 0.5001131460906784\n",
      "iteration 0 / 1: loss 0.5000278605420756\n",
      "iteration 0 / 1: loss 0.5001331820298981\n",
      "iteration 0 / 1: loss 0.5000560566954695\n",
      "iteration 0 / 1: loss 0.5000888409846236\n",
      "iteration 0 / 1: loss 0.5000441023944576\n",
      "iteration 0 / 1: loss 0.5000256097966921\n",
      "iteration 0 / 1: loss 0.49995780452603744\n",
      "iteration 0 / 1: loss 0.5000476062392107\n",
      "iteration 0 / 1: loss 0.5000088218711743\n",
      "iteration 0 / 1: loss 0.5000421759841822\n",
      "iteration 0 / 1: loss 0.5000368813845514\n",
      "iteration 0 / 1: loss 0.5000018604533305\n",
      "iteration 0 / 1: loss 0.5000321249970807\n",
      "iteration 0 / 1: loss 0.5000604854051305\n",
      "iteration 0 / 1: loss 0.500046933620619\n",
      "iteration 0 / 1: loss 0.500068391668094\n",
      "iteration 0 / 1: loss 0.5000037292006225\n",
      "iteration 0 / 1: loss 0.5000047286605505\n",
      "iteration 0 / 1: loss 0.5000525144602613\n",
      "iteration 0 / 1: loss 0.49999962803748954\n",
      "iteration 0 / 1: loss 0.5000316190927099\n",
      "iteration 0 / 1: loss 0.5000799642205458\n",
      "iteration 0 / 1: loss 0.500034787775896\n",
      "iteration 0 / 1: loss 0.500042436047946\n",
      "iteration 0 / 1: loss 0.500041915149979\n",
      "iteration 0 / 1: loss 0.500044447299377\n",
      "iteration 0 / 1: loss 0.5000200826028879\n",
      "iteration 0 / 1: loss 0.5000392704918081\n",
      "iteration 0 / 1: loss 0.5000169954828879\n",
      "iteration 0 / 1: loss 0.5000388424133496\n",
      "iteration 0 / 1: loss 0.5000199640831554\n",
      "iteration 0 / 1: loss 0.5000171127030664\n",
      "iteration 0 / 1: loss 0.4999906858876144\n",
      "iteration 0 / 1: loss 0.500041827884485\n",
      "iteration 0 / 1: loss 0.5000128107562384\n",
      "iteration 0 / 1: loss 0.5000197393138538\n",
      "iteration 0 / 1: loss 0.5000152267949602\n",
      "iteration 0 / 1: loss 0.5000413378388752\n",
      "iteration 0 / 1: loss 0.5000327420720346\n",
      "iteration 0 / 1: loss 0.4999804081968808\n",
      "iteration 0 / 1: loss 0.49997060655997067\n",
      "iteration 0 / 1: loss 0.500010432767919\n",
      "iteration 0 / 1: loss 0.5000211400063544\n",
      "iteration 0 / 1: loss 0.5000115572138515\n",
      "iteration 0 / 1: loss 0.5000005781622345\n",
      "iteration 0 / 1: loss 0.5000208501460014\n",
      "iteration 0 / 1: loss 0.5000448851015685\n",
      "iteration 0 / 1: loss 0.50007405197302\n",
      "iteration 0 / 1: loss 0.5000255095807842\n",
      "iteration 0 / 1: loss 0.5000089530442564\n",
      "iteration 0 / 1: loss 0.5000508385834102\n",
      "iteration 0 / 1: loss 0.5000212199316764\n",
      "iteration 0 / 1: loss 0.5000353343731453\n",
      "iteration 0 / 1: loss 0.50003458554307\n",
      "iteration 0 / 1: loss 0.5000365054164635\n",
      "iteration 0 / 1: loss 0.5000297949305228\n",
      "iteration 0 / 1: loss 0.5000229407002724\n",
      "iteration 0 / 1: loss 0.500058595040636\n",
      "iteration 0 / 1: loss 0.500023778707254\n",
      "iteration 0 / 1: loss 0.5000703494626191\n",
      "iteration 0 / 1: loss 0.5000399906129576\n",
      "iteration 0 / 1: loss 0.49999888070825543\n",
      "iteration 0 / 1: loss 0.5000331092278099\n",
      "iteration 0 / 1: loss 0.4999800571554836\n",
      "iteration 0 / 1: loss 0.5000176650083328\n",
      "iteration 0 / 1: loss 0.5000302198282663\n",
      "iteration 0 / 1: loss 0.4999947895634747\n",
      "iteration 0 / 1: loss 0.5000401793915391\n",
      "iteration 0 / 1: loss 0.5000273681504184\n",
      "iteration 0 / 1: loss 0.5000090812503577\n",
      "iteration 0 / 1: loss 0.500049533171517\n",
      "iteration 0 / 1: loss 0.5000354226797248\n",
      "iteration 0 / 1: loss 0.5000487504231749\n",
      "iteration 0 / 1: loss 0.5000298433447078\n",
      "iteration 0 / 1: loss 0.4999987952710518\n",
      "iteration 0 / 1: loss 0.500055969603708\n",
      "iteration 0 / 1: loss 0.500069790699245\n",
      "iteration 0 / 1: loss 0.4999759308243893\n",
      "iteration 0 / 1: loss 0.5000615599757067\n",
      "iteration 0 / 1: loss 0.49999748809994754\n",
      "iteration 0 / 1: loss 0.49998515313100395\n",
      "iteration 0 / 1: loss 0.5000441429589803\n",
      "iteration 0 / 1: loss 0.5000352388299043\n",
      "iteration 0 / 1: loss 0.5000330070902752\n",
      "iteration 0 / 1: loss 0.5000348960523603\n",
      "iteration 0 / 1: loss 0.5000395057927007\n",
      "iteration 0 / 1: loss 0.5000379958866655\n",
      "iteration 0 / 1: loss 0.5000171860503327\n",
      "iteration 0 / 1: loss 0.50000292713101\n",
      "iteration 0 / 1: loss 0.5000154091157629\n",
      "iteration 0 / 1: loss 0.500064708248178\n",
      "iteration 0 / 1: loss 0.5000354883811358\n",
      "iteration 0 / 1: loss 0.5000105443992582\n",
      "iteration 0 / 1: loss 0.5000189037903134\n",
      "iteration 0 / 1: loss 0.5000109867377807\n",
      "iteration 0 / 1: loss 0.4999986773455421\n",
      "iteration 0 / 1: loss 0.49998796654756755\n",
      "iteration 0 / 1: loss 0.5000265350392393\n",
      "iteration 0 / 1: loss 0.49999784822957916\n",
      "iteration 0 / 1: loss 0.49998965524242106\n",
      "iteration 0 / 1: loss 0.5000357555325636\n",
      "iteration 0 / 1: loss 0.5000721186626308\n",
      "iteration 0 / 1: loss 0.49998448836612763\n",
      "iteration 0 / 1: loss 0.5000181399633381\n",
      "iteration 0 / 1: loss 0.5000171829047796\n",
      "iteration 0 / 1: loss 0.49999709161308464\n",
      "iteration 0 / 1: loss 0.5000195163557793\n",
      "iteration 0 / 1: loss 0.4999997933951116\n",
      "iteration 0 / 1: loss 0.4999783264701281\n",
      "iteration 0 / 1: loss 0.5000151127714081\n",
      "iteration 0 / 1: loss 0.5000191162230514\n",
      "iteration 0 / 1: loss 0.500032816764691\n",
      "iteration 0 / 1: loss 0.5000150741256242\n",
      "iteration 0 / 1: loss 0.49997609088450046\n",
      "iteration 0 / 1: loss 0.5000128550613111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1: loss 0.5000369465551459\n",
      "iteration 0 / 1: loss 0.500028181290615\n",
      "iteration 0 / 1: loss 0.5000265356374617\n",
      "iteration 0 / 1: loss 0.5000226308568205\n",
      "iteration 0 / 1: loss 0.4999818314830576\n",
      "iteration 0 / 1: loss 0.50000016091705\n",
      "iteration 0 / 1: loss 0.499998410269488\n",
      "iteration 0 / 1: loss 0.5000134128275047\n",
      "iteration 0 / 1: loss 0.5000052258055598\n",
      "iteration 0 / 1: loss 0.49998854207220794\n",
      "iteration 0 / 1: loss 0.5000152806555597\n",
      "iteration 0 / 1: loss 0.5000158445770893\n",
      "iteration 0 / 1: loss 0.4999881257840513\n",
      "iteration 0 / 1: loss 0.5000291439194207\n",
      "iteration 0 / 1: loss 0.5000251127823606\n",
      "iteration 0 / 1: loss 0.500000305658583\n",
      "iteration 0 / 1: loss 0.5000369780733974\n",
      "iteration 0 / 1: loss 0.5000261011608633\n",
      "iteration 0 / 1: loss 0.49996791344970015\n",
      "iteration 0 / 1: loss 0.5000105471832778\n",
      "iteration 0 / 1: loss 0.5000147190679186\n",
      "iteration 0 / 1: loss 0.5000118016566945\n",
      "iteration 0 / 1: loss 0.5000013295039176\n",
      "iteration 0 / 1: loss 0.49999195206619784\n",
      "iteration 0 / 1: loss 0.49998122238878717\n",
      "iteration 0 / 1: loss 0.5000322659464665\n",
      "iteration 0 / 1: loss 0.5000004525231985\n",
      "iteration 0 / 1: loss 0.5000410647496948\n",
      "iteration 0 / 1: loss 0.5000005223976914\n",
      "iteration 0 / 1: loss 0.5000554063952067\n",
      "iteration 0 / 1: loss 0.49998274800976583\n",
      "iteration 0 / 1: loss 0.5000163831889597\n",
      "iteration 0 / 1: loss 0.4999870924048589\n",
      "iteration 0 / 1: loss 0.49999272602766176\n",
      "iteration 0 / 1: loss 0.49998527259991843\n",
      "iteration 0 / 1: loss 0.5000156305835635\n",
      "iteration 0 / 1: loss 0.5000051171053018\n",
      "iteration 0 / 1: loss 0.5000116130296445\n",
      "iteration 0 / 1: loss 0.49998190316045193\n",
      "iteration 0 / 1: loss 0.5000198706865875\n",
      "iteration 0 / 1: loss 0.4999998279614711\n",
      "iteration 0 / 1: loss 0.5000144856403215\n",
      "iteration 0 / 1: loss 0.4999802988632642\n",
      "iteration 0 / 1: loss 0.49999430620583424\n",
      "iteration 0 / 1: loss 0.49995097281100215\n",
      "iteration 0 / 1: loss 0.5000168989413654\n",
      "iteration 0 / 1: loss 0.5000284880984847\n",
      "iteration 0 / 1: loss 0.5000072280526754\n",
      "iteration 0 / 1: loss 0.49999070427952697\n",
      "iteration 0 / 1: loss 0.5000052741092468\n",
      "iteration 0 / 1: loss 0.5000350937753815\n",
      "iteration 0 / 1: loss 0.5000090176945708\n",
      "iteration 0 / 1: loss 0.5000115514203521\n",
      "iteration 0 / 1: loss 0.500003140562499\n",
      "iteration 0 / 1: loss 0.5000267098900797\n",
      "iteration 0 / 1: loss 0.5000082386890071\n",
      "iteration 0 / 1: loss 0.4999992203401634\n",
      "iteration 0 / 1: loss 0.5000152679706538\n",
      "iteration 0 / 1: loss 0.49998702538975165\n",
      "iteration 0 / 1: loss 0.5000209753677642\n",
      "iteration 0 / 1: loss 0.499993741419971\n",
      "iteration 0 / 1: loss 0.5000346411394173\n",
      "iteration 0 / 1: loss 0.5000020258203323\n",
      "iteration 0 / 1: loss 0.49997602613705383\n",
      "iteration 0 / 1: loss 0.49999589425806223\n",
      "iteration 0 / 1: loss 0.5000040675149213\n",
      "iteration 0 / 1: loss 0.5000278898039224\n",
      "iteration 0 / 1: loss 0.5000513712128457\n",
      "iteration 0 / 1: loss 0.5000028815387344\n",
      "iteration 0 / 1: loss 0.4999791541905336\n",
      "iteration 0 / 1: loss 0.5000121066693342\n",
      "iteration 0 / 1: loss 0.49997615027168174\n",
      "iteration 0 / 1: loss 0.5000203624059932\n",
      "iteration 0 / 1: loss 0.49999844249966613\n",
      "iteration 0 / 1: loss 0.4999760246231664\n",
      "iteration 0 / 1: loss 0.5000205781742474\n",
      "iteration 0 / 1: loss 0.500001753689262\n",
      "iteration 0 / 1: loss 0.5000052729440305\n",
      "iteration 0 / 1: loss 0.49999675984066955\n",
      "iteration 0 / 1: loss 0.499999678421217\n",
      "iteration 0 / 1: loss 0.49998754902289877\n",
      "iteration 0 / 1: loss 0.5000163943069469\n",
      "iteration 0 / 1: loss 0.5000096750950517\n",
      "iteration 0 / 1: loss 0.500002869965885\n",
      "iteration 0 / 1: loss 0.5000225731902996\n",
      "iteration 0 / 1: loss 0.5000074162906515\n",
      "iteration 0 / 1: loss 0.5000047229476815\n",
      "iteration 0 / 1: loss 0.4999992126568926\n",
      "iteration 0 / 1: loss 0.5000060847930189\n",
      "iteration 0 / 1: loss 0.5000358801980344\n",
      "iteration 0 / 1: loss 0.49998124042357245\n",
      "iteration 0 / 1: loss 0.4999821424432076\n",
      "iteration 0 / 1: loss 0.4999958592859098\n",
      "iteration 0 / 1: loss 0.5000091823129241\n",
      "iteration 0 / 1: loss 0.49998961001737363\n",
      "iteration 0 / 1: loss 0.4999834623167783\n",
      "iteration 0 / 1: loss 0.500004850058894\n",
      "iteration 0 / 1: loss 0.49998313973131436\n",
      "iteration 0 / 1: loss 0.49998618606152523\n",
      "iteration 0 / 1: loss 0.500006333672036\n",
      "iteration 0 / 1: loss 0.49999081024201786\n",
      "iteration 0 / 1: loss 0.4999965249491494\n",
      "iteration 0 / 1: loss 0.4999790048140727\n",
      "iteration 0 / 1: loss 0.5000523407120744\n",
      "iteration 0 / 1: loss 0.50000937805159\n",
      "iteration 0 / 1: loss 0.5000261059461235\n",
      "iteration 0 / 1: loss 0.5000425458282419\n",
      "iteration 0 / 1: loss 0.5000127252424565\n",
      "iteration 0 / 1: loss 0.5000184907328915\n",
      "iteration 0 / 1: loss 0.5000172930499291\n",
      "iteration 0 / 1: loss 0.5000336456037109\n",
      "iteration 0 / 1: loss 0.4999687020746236\n",
      "iteration 0 / 1: loss 0.5000062508666822\n",
      "iteration 0 / 1: loss 0.4999865163107237\n",
      "iteration 0 / 1: loss 0.4999977708764183\n",
      "iteration 0 / 1: loss 0.4999960818321406\n",
      "iteration 0 / 1: loss 0.4999814897920327\n",
      "iteration 0 / 1: loss 0.4999980041325145\n",
      "iteration 0 / 1: loss 0.5000097773569798\n",
      "iteration 0 / 1: loss 0.5000037085371194\n",
      "iteration 0 / 1: loss 0.4999972609829542\n",
      "iteration 0 / 1: loss 0.500011559301409\n",
      "iteration 0 / 1: loss 0.4999970511784689\n",
      "iteration 0 / 1: loss 0.49999966462458934\n",
      "iteration 0 / 1: loss 0.4999859315966466\n",
      "iteration 0 / 1: loss 0.5000107844088836\n",
      "iteration 0 / 1: loss 0.5000076662833577\n",
      "iteration 0 / 1: loss 0.5000202820807679\n",
      "iteration 0 / 1: loss 0.49999363022081117\n",
      "iteration 0 / 1: loss 0.4999786022380447\n",
      "iteration 0 / 1: loss 0.49998421565930684\n",
      "iteration 0 / 1: loss 0.4999979314658096\n",
      "iteration 0 / 1: loss 0.4999830507088722\n",
      "iteration 0 / 1: loss 0.5000038743986256\n",
      "iteration 0 / 1: loss 0.4999645249633825\n",
      "iteration 0 / 1: loss 0.4999782546587944\n",
      "iteration 0 / 1: loss 0.49999038050811706\n",
      "iteration 0 / 1: loss 0.5000317983055514\n",
      "iteration 0 / 1: loss 0.4999758703502201\n",
      "iteration 0 / 1: loss 0.49997939756574256\n",
      "iteration 0 / 1: loss 0.4999952256226033\n",
      "iteration 0 / 1: loss 0.49998187906683206\n",
      "iteration 0 / 1: loss 0.49998578622498646\n",
      "iteration 0 / 1: loss 0.5000442608369834\n",
      "iteration 0 / 1: loss 0.49998177530601196\n",
      "iteration 0 / 1: loss 0.4999778674217786\n",
      "iteration 0 / 1: loss 0.500021526097145\n",
      "iteration 0 / 1: loss 0.4999821074407292\n",
      "iteration 0 / 1: loss 0.49997752988701555\n",
      "iteration 0 / 1: loss 0.49997157073900894\n",
      "iteration 0 / 1: loss 0.49998605496991655\n",
      "iteration 0 / 1: loss 0.5000099970959416\n",
      "iteration 0 / 1: loss 0.5000121465398338\n",
      "iteration 0 / 1: loss 0.5000095924139906\n",
      "iteration 0 / 1: loss 0.4999926978643683\n",
      "iteration 0 / 1: loss 0.4999800553276621\n",
      "iteration 0 / 1: loss 0.499988286845111\n",
      "iteration 0 / 1: loss 0.49998246955576203\n",
      "iteration 0 / 1: loss 0.49997618517907155\n",
      "iteration 0 / 1: loss 0.4999632883436688\n",
      "iteration 0 / 1: loss 0.49997695103154643\n",
      "iteration 0 / 1: loss 0.4999681419784361\n",
      "iteration 0 / 1: loss 0.4999647039122901\n",
      "iteration 0 / 1: loss 0.49999002422013955\n",
      "iteration 0 / 1: loss 0.4999962485365417\n",
      "iteration 0 / 1: loss 0.49999425748377535\n",
      "iteration 0 / 1: loss 0.4999658883317773\n",
      "iteration 0 / 1: loss 0.4999817211999717\n",
      "iteration 0 / 1: loss 0.5000042009266805\n",
      "iteration 0 / 1: loss 0.5000026439887482\n",
      "iteration 0 / 1: loss 0.5000069902692933\n",
      "iteration 0 / 1: loss 0.5000096890942088\n",
      "iteration 0 / 1: loss 0.49998533355096336\n",
      "iteration 0 / 1: loss 0.4999783838215884\n",
      "iteration 0 / 1: loss 0.5000007671703968\n",
      "iteration 0 / 1: loss 0.49997676929555634\n",
      "iteration 0 / 1: loss 0.5000019669811002\n",
      "iteration 0 / 1: loss 0.5000015376699999\n",
      "iteration 0 / 1: loss 0.49998908032765615\n",
      "iteration 0 / 1: loss 0.4999989325154568\n",
      "iteration 0 / 1: loss 0.499989895270149\n",
      "iteration 0 / 1: loss 0.5000018122644154\n",
      "iteration 0 / 1: loss 0.5000097009162265\n",
      "iteration 0 / 1: loss 0.49999305158451757\n",
      "iteration 0 / 1: loss 0.5000145267406846\n",
      "iteration 0 / 1: loss 0.4999919757444658\n",
      "iteration 0 / 1: loss 0.49999584393294344\n",
      "iteration 0 / 1: loss 0.4999853067504364\n",
      "iteration 0 / 1: loss 0.5000067250838397\n",
      "iteration 0 / 1: loss 0.4999963728635636\n",
      "iteration 0 / 1: loss 0.4999792370839246\n",
      "iteration 0 / 1: loss 0.499997932849771\n",
      "iteration 0 / 1: loss 0.5000156435086558\n",
      "iteration 0 / 1: loss 0.5000026790283878\n",
      "iteration 0 / 1: loss 0.4999908066210847\n",
      "iteration 0 / 1: loss 0.49997878358152076\n",
      "iteration 0 / 1: loss 0.500014646822308\n",
      "iteration 0 / 1: loss 0.500004096223764\n",
      "iteration 0 / 1: loss 0.4999799571642814\n",
      "iteration 0 / 1: loss 0.49996920762749847\n",
      "iteration 0 / 1: loss 0.5000078959045104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1: loss 0.49999233488579375\n",
      "iteration 0 / 1: loss 0.4999876628500196\n",
      "iteration 0 / 1: loss 0.5000068282597968\n",
      "iteration 0 / 1: loss 0.49997710137765716\n",
      "iteration 0 / 1: loss 0.49998556789116705\n",
      "iteration 0 / 1: loss 0.5000038858079843\n",
      "iteration 0 / 1: loss 0.4999943999947729\n",
      "iteration 0 / 1: loss 0.49998924497321134\n",
      "iteration 0 / 1: loss 0.5000053012552017\n",
      "iteration 0 / 1: loss 0.49998972677000114\n",
      "iteration 0 / 1: loss 0.5000098645893131\n",
      "iteration 0 / 1: loss 0.5000050987373521\n",
      "iteration 0 / 1: loss 0.5000157326190074\n",
      "iteration 0 / 1: loss 0.5000073107062125\n",
      "iteration 0 / 1: loss 0.4999801352451316\n",
      "iteration 0 / 1: loss 0.5000078732623862\n",
      "iteration 0 / 1: loss 0.5000075921664087\n",
      "iteration 0 / 1: loss 0.5000017250301706\n",
      "iteration 0 / 1: loss 0.5000120452892727\n",
      "iteration 0 / 1: loss 0.499979994165847\n",
      "iteration 0 / 1: loss 0.49998786257154443\n",
      "iteration 0 / 1: loss 0.4999885035911511\n",
      "iteration 0 / 1: loss 0.49998755533943046\n",
      "iteration 0 / 1: loss 0.4999786736465787\n",
      "iteration 0 / 1: loss 0.4999857042208248\n",
      "iteration 0 / 1: loss 0.4999916318422508\n",
      "iteration 0 / 1: loss 0.4999915227628405\n",
      "iteration 0 / 1: loss 0.499969308794282\n",
      "iteration 0 / 1: loss 0.4999885031889042\n",
      "iteration 0 / 1: loss 0.499986568418978\n",
      "iteration 0 / 1: loss 0.4999771858524865\n",
      "iteration 0 / 1: loss 0.4999935523004784\n",
      "iteration 0 / 1: loss 0.49998270110871174\n",
      "iteration 0 / 1: loss 0.5000063490589152\n",
      "iteration 0 / 1: loss 0.4999962038534197\n",
      "iteration 0 / 1: loss 0.5000078640088544\n",
      "iteration 0 / 1: loss 0.4999859924031545\n",
      "iteration 0 / 1: loss 0.4999965923277234\n",
      "iteration 0 / 1: loss 0.4999866473013088\n",
      "iteration 0 / 1: loss 0.49997923352660006\n",
      "iteration 0 / 1: loss 0.49999755948900193\n",
      "iteration 0 / 1: loss 0.4999912163756059\n",
      "iteration 0 / 1: loss 0.4999773731146764\n",
      "iteration 0 / 1: loss 0.49999210853777853\n",
      "iteration 0 / 1: loss 0.4999796008688675\n",
      "iteration 0 / 1: loss 0.4999802756091437\n",
      "iteration 0 / 1: loss 0.49997589761922107\n",
      "iteration 0 / 1: loss 0.49996703545913396\n",
      "iteration 0 / 1: loss 0.5000000393695486\n",
      "iteration 0 / 1: loss 0.5000063171959589\n",
      "iteration 0 / 1: loss 0.49999191385595376\n",
      "iteration 0 / 1: loss 0.4999901267600991\n",
      "iteration 0 / 1: loss 0.4999925531545027\n",
      "iteration 0 / 1: loss 0.49998789339534794\n",
      "iteration 0 / 1: loss 0.4999951641797246\n",
      "iteration 0 / 1: loss 0.4999771996598491\n",
      "iteration 0 / 1: loss 0.4999800050730827\n",
      "iteration 0 / 1: loss 0.49999459409876834\n",
      "iteration 0 / 1: loss 0.4999880007948065\n",
      "iteration 0 / 1: loss 0.4999828866031003\n",
      "iteration 0 / 1: loss 0.4999781419090909\n",
      "iteration 0 / 1: loss 0.49998430563366425\n",
      "iteration 0 / 1: loss 0.499969675203376\n",
      "iteration 0 / 1: loss 0.49997778022244505\n",
      "iteration 0 / 1: loss 0.4999973241468128\n",
      "iteration 0 / 1: loss 0.49998691145611324\n",
      "iteration 0 / 1: loss 0.49997820417526523\n",
      "iteration 0 / 1: loss 0.4999662886863896\n",
      "iteration 0 / 1: loss 0.49998750543020315\n",
      "iteration 0 / 1: loss 0.49998847048129824\n",
      "iteration 0 / 1: loss 0.4999858325989673\n",
      "iteration 0 / 1: loss 0.4999890704155588\n",
      "iteration 0 / 1: loss 0.49999047993495277\n",
      "iteration 0 / 1: loss 0.49997057436075204\n",
      "iteration 0 / 1: loss 0.49996770540038427\n",
      "iteration 0 / 1: loss 0.4999854028067\n",
      "iteration 0 / 1: loss 0.49996932314768583\n",
      "iteration 0 / 1: loss 0.49998589397722293\n",
      "iteration 0 / 1: loss 0.49998802293630845\n",
      "iteration 0 / 1: loss 0.4999783663564074\n",
      "iteration 0 / 1: loss 0.4999740991472369\n",
      "iteration 0 / 1: loss 0.5000036205876656\n",
      "iteration 0 / 1: loss 0.4999751806594323\n",
      "iteration 0 / 1: loss 0.4999727574782028\n",
      "iteration 0 / 1: loss 0.49998826614430186\n",
      "iteration 0 / 1: loss 0.49999893252546407\n",
      "iteration 0 / 1: loss 0.49997285241494727\n",
      "iteration 0 / 1: loss 0.49998779080101136\n",
      "iteration 0 / 1: loss 0.49998153958692704\n",
      "iteration 0 / 1: loss 0.500009075762583\n",
      "iteration 0 / 1: loss 0.4999870144215765\n",
      "iteration 0 / 1: loss 0.4999877931993928\n",
      "iteration 0 / 1: loss 0.4999745331091666\n",
      "iteration 0 / 1: loss 0.49999243251552267\n",
      "iteration 0 / 1: loss 0.499990860587436\n",
      "iteration 0 / 1: loss 0.4999753806934752\n",
      "iteration 0 / 1: loss 0.49997770068638825\n",
      "iteration 0 / 1: loss 0.49999454129865606\n",
      "iteration 0 / 1: loss 0.4999831074127313\n",
      "iteration 0 / 1: loss 0.499964279552148\n",
      "iteration 0 / 1: loss 0.4999828610814759\n",
      "iteration 0 / 1: loss 0.500008454905441\n",
      "iteration 0 / 1: loss 0.4999795106108583\n",
      "iteration 0 / 1: loss 0.49996585193002835\n",
      "iteration 0 / 1: loss 0.4999938523485692\n",
      "iteration 0 / 1: loss 0.4999814705969898\n",
      "iteration 0 / 1: loss 0.49999185079631336\n",
      "iteration 0 / 1: loss 0.49997958869415215\n",
      "iteration 0 / 1: loss 0.49996837828047064\n",
      "iteration 0 / 1: loss 0.4999867387576371\n",
      "iteration 0 / 1: loss 0.49998952851556694\n",
      "iteration 0 / 1: loss 0.4999849302005894\n",
      "iteration 0 / 1: loss 0.4999799227009172\n",
      "iteration 0 / 1: loss 0.4999735018344063\n",
      "iteration 0 / 1: loss 0.4999859029486555\n",
      "iteration 0 / 1: loss 0.49997236716448523\n",
      "iteration 0 / 1: loss 0.499980892303019\n",
      "iteration 0 / 1: loss 0.49998052322972547\n",
      "iteration 0 / 1: loss 0.49998707893106287\n",
      "iteration 0 / 1: loss 0.500009757144341\n",
      "iteration 0 / 1: loss 0.5000038815711275\n",
      "iteration 0 / 1: loss 0.49999102197087414\n",
      "iteration 0 / 1: loss 0.49998127560668765\n",
      "iteration 0 / 1: loss 0.49997418845073277\n",
      "iteration 0 / 1: loss 0.49998359768492995\n",
      "iteration 0 / 1: loss 0.49999862727532\n",
      "iteration 0 / 1: loss 0.4999748448296758\n",
      "iteration 0 / 1: loss 0.4999691104131165\n",
      "iteration 0 / 1: loss 0.49994396318729084\n",
      "iteration 0 / 1: loss 0.49998729342478215\n",
      "iteration 0 / 1: loss 0.4999754477970532\n",
      "iteration 0 / 1: loss 0.4999768822051926\n",
      "iteration 0 / 1: loss 0.4999565631853787\n",
      "iteration 0 / 1: loss 0.4999808820025442\n",
      "iteration 0 / 1: loss 0.49998946142491363\n",
      "iteration 0 / 1: loss 0.4999756108280194\n",
      "iteration 0 / 1: loss 0.49996285456873374\n",
      "iteration 0 / 1: loss 0.49998220591899584\n",
      "iteration 0 / 1: loss 0.499983056043496\n",
      "iteration 0 / 1: loss 0.4999999120459919\n",
      "iteration 0 / 1: loss 0.49997240641345114\n",
      "iteration 0 / 1: loss 0.49995671175331946\n",
      "iteration 0 / 1: loss 0.4999784495847099\n",
      "iteration 0 / 1: loss 0.49998135979920166\n",
      "iteration 0 / 1: loss 0.49994486325681775\n",
      "iteration 0 / 1: loss 0.4999870728089517\n",
      "iteration 0 / 1: loss 0.49997571840879956\n",
      "iteration 0 / 1: loss 0.49997279581037446\n",
      "iteration 0 / 1: loss 0.4999764745706516\n",
      "iteration 0 / 1: loss 0.4999896283283037\n",
      "iteration 0 / 1: loss 0.4999598455116139\n",
      "iteration 0 / 1: loss 0.4999880240419124\n",
      "iteration 0 / 1: loss 0.4999674031327187\n",
      "iteration 0 / 1: loss 0.49997951091985066\n",
      "iteration 0 / 1: loss 0.4999614186697572\n",
      "iteration 0 / 1: loss 0.49998378975858465\n",
      "iteration 0 / 1: loss 0.49997497085245113\n",
      "iteration 0 / 1: loss 0.4999716675761866\n",
      "iteration 0 / 1: loss 0.499966752956931\n",
      "iteration 0 / 1: loss 0.49999078032190697\n",
      "iteration 0 / 1: loss 0.4999840181886224\n",
      "iteration 0 / 1: loss 0.49998209066144966\n",
      "iteration 0 / 1: loss 0.49998878644438444\n",
      "iteration 0 / 1: loss 0.49998019398060856\n",
      "iteration 0 / 1: loss 0.49998247446242533\n",
      "iteration 0 / 1: loss 0.4999641662312832\n",
      "iteration 0 / 1: loss 0.49997857278231\n",
      "iteration 0 / 1: loss 0.4999713210198187\n",
      "iteration 0 / 1: loss 0.499983806290363\n",
      "iteration 0 / 1: loss 0.4999770313579904\n",
      "iteration 0 / 1: loss 0.49998002631382527\n",
      "iteration 0 / 1: loss 0.49998923772275394\n",
      "iteration 0 / 1: loss 0.49994907982607206\n",
      "iteration 0 / 1: loss 0.4999637620238332\n",
      "iteration 0 / 1: loss 0.49998696447632135\n",
      "iteration 0 / 1: loss 0.4999699399376\n",
      "iteration 0 / 1: loss 0.4999662996287788\n",
      "iteration 0 / 1: loss 0.49995909448471687\n",
      "iteration 0 / 1: loss 0.49998513255374044\n",
      "iteration 0 / 1: loss 0.4999673024637988\n",
      "iteration 0 / 1: loss 0.49998052159954237\n",
      "iteration 0 / 1: loss 0.49997157458719815\n",
      "iteration 0 / 1: loss 0.49999311614507425\n",
      "iteration 0 / 1: loss 0.4999583769201789\n",
      "iteration 0 / 1: loss 0.49997030015548444\n",
      "iteration 0 / 1: loss 0.4999695124060708\n",
      "iteration 0 / 1: loss 0.49996739791446987\n",
      "iteration 0 / 1: loss 0.49997794080526725\n",
      "iteration 0 / 1: loss 0.4999780232381115\n",
      "iteration 0 / 1: loss 0.4999554636861542\n",
      "iteration 0 / 1: loss 0.49996878700540676\n",
      "iteration 0 / 1: loss 0.4999857146196813\n",
      "iteration 0 / 1: loss 0.49996657986545484\n",
      "iteration 0 / 1: loss 0.499963840799038\n",
      "iteration 0 / 1: loss 0.49995994591244153\n",
      "iteration 0 / 1: loss 0.49997744520482373\n",
      "iteration 0 / 1: loss 0.49996378597890523\n",
      "iteration 0 / 1: loss 0.4999803883251216\n",
      "iteration 0 / 1: loss 0.49997849953519946\n",
      "iteration 0 / 1: loss 0.49998421546269395\n",
      "iteration 0 / 1: loss 0.49993977441205734\n",
      "iteration 0 / 1: loss 0.49997678607198687\n",
      "iteration 0 / 1: loss 0.4999658379512876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1: loss 0.4999815733636927\n",
      "iteration 0 / 1: loss 0.49996240370322553\n",
      "iteration 0 / 1: loss 0.4999611573556925\n",
      "iteration 0 / 1: loss 0.4999744949262954\n",
      "iteration 0 / 1: loss 0.4999740743073912\n",
      "iteration 0 / 1: loss 0.4999639988897083\n",
      "iteration 0 / 1: loss 0.49996006657257913\n",
      "iteration 0 / 1: loss 0.49994797548125136\n",
      "iteration 0 / 1: loss 0.49997260492962103\n",
      "iteration 0 / 1: loss 0.49998279466465784\n",
      "iteration 0 / 1: loss 0.4999633010278313\n",
      "iteration 0 / 1: loss 0.4999728869224912\n",
      "iteration 0 / 1: loss 0.4999368741457015\n",
      "iteration 0 / 1: loss 0.4999849394305921\n",
      "iteration 0 / 1: loss 0.4999823158954644\n",
      "iteration 0 / 1: loss 0.4999570222276265\n",
      "iteration 0 / 1: loss 0.49996068152270506\n",
      "iteration 0 / 1: loss 0.4999501306559205\n",
      "iteration 0 / 1: loss 0.499960319518253\n",
      "iteration 0 / 1: loss 0.49996185192491954\n",
      "iteration 0 / 1: loss 0.49996902808304206\n",
      "iteration 0 / 1: loss 0.4999573750722786\n",
      "iteration 0 / 1: loss 0.4999607892585195\n",
      "iteration 0 / 1: loss 0.4999555898714688\n",
      "iteration 0 / 1: loss 0.49996827016505374\n",
      "iteration 0 / 1: loss 0.4999707133185099\n",
      "iteration 0 / 1: loss 0.4999711752756428\n",
      "iteration 0 / 1: loss 0.4999640198426542\n",
      "iteration 0 / 1: loss 0.4999778473510057\n",
      "iteration 0 / 1: loss 0.49997685164700717\n",
      "iteration 0 / 1: loss 0.4999684482951072\n",
      "iteration 0 / 1: loss 0.49996578686345966\n",
      "iteration 0 / 1: loss 0.4999702881375086\n",
      "iteration 0 / 1: loss 0.4999575368636475\n",
      "iteration 0 / 1: loss 0.4999595804182645\n",
      "iteration 0 / 1: loss 0.4999651487081051\n",
      "iteration 0 / 1: loss 0.4999358059050886\n",
      "iteration 0 / 1: loss 0.49995636733783694\n",
      "iteration 0 / 1: loss 0.49996076569192555\n",
      "iteration 0 / 1: loss 0.49998281820318025\n",
      "iteration 0 / 1: loss 0.499960089240941\n",
      "iteration 0 / 1: loss 0.49996857262914285\n",
      "iteration 0 / 1: loss 0.49997034153663916\n",
      "iteration 0 / 1: loss 0.49997880867176553\n",
      "iteration 0 / 1: loss 0.4999430240179195\n",
      "iteration 0 / 1: loss 0.4999624419967555\n",
      "iteration 0 / 1: loss 0.49995384438272905\n",
      "iteration 0 / 1: loss 0.4999657776964556\n",
      "iteration 0 / 1: loss 0.4999780544421949\n",
      "iteration 0 / 1: loss 0.4999723563406724\n",
      "iteration 0 / 1: loss 0.4999507173766553\n",
      "iteration 0 / 1: loss 0.49996716972516825\n",
      "iteration 0 / 1: loss 0.49996484205688446\n",
      "iteration 0 / 1: loss 0.49997090671433175\n",
      "iteration 0 / 1: loss 0.49996460014213284\n",
      "iteration 0 / 1: loss 0.49997261457424014\n",
      "iteration 0 / 1: loss 0.4999639085220364\n",
      "iteration 0 / 1: loss 0.4999638244475243\n",
      "iteration 0 / 1: loss 0.49996595362426144\n",
      "iteration 0 / 1: loss 0.49996136051184675\n",
      "iteration 0 / 1: loss 0.49996349136798557\n",
      "iteration 0 / 1: loss 0.49995924649547674\n",
      "iteration 0 / 1: loss 0.4999496930757919\n",
      "iteration 0 / 1: loss 0.4999571138803778\n",
      "iteration 0 / 1: loss 0.4999676462444581\n",
      "iteration 0 / 1: loss 0.4999640314241084\n",
      "iteration 0 / 1: loss 0.4999626921589702\n",
      "iteration 0 / 1: loss 0.49995049283725723\n",
      "iteration 0 / 1: loss 0.4999581371275068\n",
      "iteration 0 / 1: loss 0.4999554497167885\n",
      "iteration 0 / 1: loss 0.4999591868432815\n",
      "iteration 0 / 1: loss 0.4999659483816281\n",
      "iteration 0 / 1: loss 0.4999354747545781\n",
      "iteration 0 / 1: loss 0.49996552658461946\n",
      "iteration 0 / 1: loss 0.4999662702960824\n",
      "iteration 0 / 1: loss 0.4999790211979533\n",
      "iteration 0 / 1: loss 0.4999576842557287\n",
      "iteration 0 / 1: loss 0.4999472822425973\n",
      "iteration 0 / 1: loss 0.49994784274533627\n",
      "iteration 0 / 1: loss 0.49996025508161457\n",
      "iteration 0 / 1: loss 0.4999663826032668\n",
      "iteration 0 / 1: loss 0.49997750955985765\n",
      "iteration 0 / 1: loss 0.49996249045532765\n",
      "iteration 0 / 1: loss 0.49995255431051294\n",
      "iteration 0 / 1: loss 0.4999705208912987\n",
      "iteration 0 / 1: loss 0.4999486074907978\n",
      "iteration 0 / 1: loss 0.49995444862608873\n",
      "iteration 0 / 1: loss 0.4999511696830745\n",
      "iteration 0 / 1: loss 0.49996716210080727\n",
      "iteration 0 / 1: loss 0.4999499263673271\n",
      "iteration 0 / 1: loss 0.49996275833096226\n",
      "iteration 0 / 1: loss 0.4999530031325999\n",
      "iteration 0 / 1: loss 0.49995975738399767\n",
      "iteration 0 / 1: loss 0.4999466546716565\n",
      "iteration 0 / 1: loss 0.4999545996398075\n",
      "iteration 0 / 1: loss 0.4999566807573176\n",
      "iteration 0 / 1: loss 0.4999669344585775\n",
      "iteration 0 / 1: loss 0.4999451496553969\n",
      "iteration 0 / 1: loss 0.499983590432552\n",
      "iteration 0 / 1: loss 0.49994773006568943\n",
      "iteration 0 / 1: loss 0.4999641441757456\n",
      "iteration 0 / 1: loss 0.49997461255487113\n",
      "iteration 0 / 1: loss 0.49996171729425487\n",
      "iteration 0 / 1: loss 0.4999598990395734\n",
      "iteration 0 / 1: loss 0.4999560811808119\n",
      "iteration 0 / 1: loss 0.49995710716421765\n",
      "iteration 0 / 1: loss 0.49995378116356376\n",
      "iteration 0 / 1: loss 0.49995597201949105\n",
      "iteration 0 / 1: loss 0.49996331635946784\n",
      "iteration 0 / 1: loss 0.49996236926858173\n",
      "iteration 0 / 1: loss 0.49995761888708334\n",
      "iteration 0 / 1: loss 0.49994495144823425\n",
      "iteration 0 / 1: loss 0.49997795024455677\n",
      "iteration 0 / 1: loss 0.49995536375461685\n",
      "iteration 0 / 1: loss 0.49996337889302406\n",
      "iteration 0 / 1: loss 0.49997232994373614\n",
      "iteration 0 / 1: loss 0.49996822394952556\n",
      "iteration 0 / 1: loss 0.49995621991084127\n",
      "iteration 0 / 1: loss 0.4999508506621533\n",
      "iteration 0 / 1: loss 0.4999604397601349\n",
      "iteration 0 / 1: loss 0.499959307438474\n",
      "iteration 0 / 1: loss 0.49995791819328916\n",
      "iteration 0 / 1: loss 0.4999533423883582\n",
      "iteration 0 / 1: loss 0.4999438754122064\n",
      "iteration 0 / 1: loss 0.49993915045080395\n",
      "iteration 0 / 1: loss 0.49996127965759546\n",
      "iteration 0 / 1: loss 0.4999522812129183\n",
      "iteration 0 / 1: loss 0.49995446224062995\n",
      "iteration 0 / 1: loss 0.49995135448419803\n",
      "iteration 0 / 1: loss 0.49996590225293647\n",
      "iteration 0 / 1: loss 0.49995808132728053\n",
      "iteration 0 / 1: loss 0.4999614290742811\n",
      "iteration 0 / 1: loss 0.49995533492399985\n",
      "iteration 0 / 1: loss 0.4999590270172191\n",
      "iteration 0 / 1: loss 0.49995860083924976\n",
      "iteration 0 / 1: loss 0.4999457185136716\n",
      "iteration 0 / 1: loss 0.4999686726761058\n",
      "iteration 0 / 1: loss 0.4999418945256497\n",
      "iteration 0 / 1: loss 0.49994791472327915\n",
      "iteration 0 / 1: loss 0.49993776052413375\n",
      "iteration 0 / 1: loss 0.4999278928345088\n",
      "iteration 0 / 1: loss 0.4999621271791521\n",
      "iteration 0 / 1: loss 0.4999607913079466\n",
      "iteration 0 / 1: loss 0.49993942469080566\n",
      "iteration 0 / 1: loss 0.4999374272045463\n",
      "iteration 0 / 1: loss 0.49995488469532956\n",
      "iteration 0 / 1: loss 0.49994111735384217\n",
      "iteration 0 / 1: loss 0.4999578648812306\n",
      "iteration 0 / 1: loss 0.49995635407197925\n",
      "iteration 0 / 1: loss 0.4999610587101221\n",
      "iteration 0 / 1: loss 0.49996428982271823\n",
      "iteration 0 / 1: loss 0.49995849147784116\n",
      "iteration 0 / 1: loss 0.4999529029976707\n",
      "iteration 0 / 1: loss 0.49995521699166134\n",
      "iteration 0 / 1: loss 0.49995408120082463\n",
      "iteration 0 / 1: loss 0.499954693518646\n",
      "iteration 0 / 1: loss 0.49996075615245755\n",
      "iteration 0 / 1: loss 0.49995693426991616\n",
      "iteration 0 / 1: loss 0.49996370476537877\n",
      "iteration 0 / 1: loss 0.49994573994473684\n",
      "iteration 0 / 1: loss 0.49996156847030687\n",
      "iteration 0 / 1: loss 0.49993489014467657\n",
      "iteration 0 / 1: loss 0.4999277091162564\n",
      "iteration 0 / 1: loss 0.4999454029969056\n",
      "iteration 0 / 1: loss 0.49994702393259527\n",
      "iteration 0 / 1: loss 0.499965681819626\n",
      "iteration 0 / 1: loss 0.49995194254740827\n",
      "iteration 0 / 1: loss 0.49994822402751043\n",
      "iteration 0 / 1: loss 0.49995106201770406\n",
      "iteration 0 / 1: loss 0.49995726773567367\n",
      "iteration 0 / 1: loss 0.49994178236101894\n",
      "iteration 0 / 1: loss 0.4999312725460596\n",
      "iteration 0 / 1: loss 0.4999493911677646\n",
      "iteration 0 / 1: loss 0.4999306201477568\n",
      "iteration 0 / 1: loss 0.49996677683221163\n",
      "iteration 0 / 1: loss 0.49994905858483996\n",
      "iteration 0 / 1: loss 0.49994914384265576\n",
      "iteration 0 / 1: loss 0.4999614062072461\n",
      "iteration 0 / 1: loss 0.49997311661757954\n",
      "iteration 0 / 1: loss 0.49996053162994114\n",
      "iteration 0 / 1: loss 0.49993536097338676\n",
      "iteration 0 / 1: loss 0.49995347643566584\n",
      "iteration 0 / 1: loss 0.4999445593190127\n",
      "iteration 0 / 1: loss 0.49995101749397935\n",
      "iteration 0 / 1: loss 0.49994196781155986\n",
      "iteration 0 / 1: loss 0.4999424699574128\n",
      "iteration 0 / 1: loss 0.4999405469229084\n",
      "iteration 0 / 1: loss 0.4999646243252969\n",
      "iteration 0 / 1: loss 0.4999465139116686\n",
      "iteration 0 / 1: loss 0.4999587504650973\n",
      "iteration 0 / 1: loss 0.49996094607698455\n",
      "iteration 0 / 1: loss 0.49992905588209186\n",
      "iteration 0 / 1: loss 0.49996025880785155\n",
      "iteration 0 / 1: loss 0.4999441807146409\n",
      "iteration 0 / 1: loss 0.4999378508039236\n",
      "iteration 0 / 1: loss 0.4999617802896537\n",
      "iteration 0 / 1: loss 0.4999408844044566\n",
      "iteration 0 / 1: loss 0.49994844665891897\n",
      "iteration 0 / 1: loss 0.49994798192031004\n",
      "iteration 0 / 1: loss 0.49995890847206387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1: loss 0.49996426845056685\n",
      "iteration 0 / 1: loss 0.49995483534132457\n",
      "iteration 0 / 1: loss 0.4999474211959463\n",
      "iteration 0 / 1: loss 0.49994199160370156\n",
      "iteration 0 / 1: loss 0.4999245410784466\n",
      "iteration 0 / 1: loss 0.4999624181268397\n",
      "iteration 0 / 1: loss 0.4999496591150037\n",
      "iteration 0 / 1: loss 0.49992910064172474\n",
      "iteration 0 / 1: loss 0.49994243778135583\n",
      "iteration 0 / 1: loss 0.49992620365274754\n",
      "iteration 0 / 1: loss 0.49993618670586293\n",
      "iteration 0 / 1: loss 0.4999303986360691\n",
      "iteration 0 / 1: loss 0.4999490476148898\n",
      "iteration 0 / 1: loss 0.49995367026855414\n",
      "iteration 0 / 1: loss 0.49993383230087024\n",
      "iteration 0 / 1: loss 0.4999438248106721\n",
      "iteration 0 / 1: loss 0.4999336233982487\n",
      "iteration 0 / 1: loss 0.4999453108550577\n",
      "iteration 0 / 1: loss 0.4999432007162911\n",
      "iteration 0 / 1: loss 0.4999496706932904\n",
      "iteration 0 / 1: loss 0.4999557634379834\n",
      "iteration 0 / 1: loss 0.4999537139809692\n",
      "iteration 0 / 1: loss 0.499953948745421\n",
      "iteration 0 / 1: loss 0.4999372506493448\n",
      "iteration 0 / 1: loss 0.499933005531208\n",
      "iteration 0 / 1: loss 0.4999513944655536\n",
      "iteration 0 / 1: loss 0.4999344334059646\n",
      "iteration 0 / 1: loss 0.4999628966806299\n",
      "iteration 0 / 1: loss 0.4999485436681568\n",
      "iteration 0 / 1: loss 0.49995871131677744\n",
      "iteration 0 / 1: loss 0.49994859165661204\n",
      "iteration 0 / 1: loss 0.4999438334106056\n",
      "iteration 0 / 1: loss 0.4999410588887898\n",
      "iteration 0 / 1: loss 0.4999489194580778\n",
      "iteration 0 / 1: loss 0.4999358151164688\n",
      "iteration 0 / 1: loss 0.49995342127602194\n",
      "iteration 0 / 1: loss 0.4999600646412483\n",
      "iteration 0 / 1: loss 0.49993761498137584\n",
      "iteration 0 / 1: loss 0.4999464710215246\n",
      "iteration 0 / 1: loss 0.4999503434684107\n",
      "iteration 0 / 1: loss 0.49995082761168996\n",
      "iteration 0 / 1: loss 0.49995240771070215\n",
      "iteration 0 / 1: loss 0.4999369661065202\n",
      "iteration 0 / 1: loss 0.4999446270753561\n",
      "iteration 0 / 1: loss 0.49994718023917145\n",
      "iteration 0 / 1: loss 0.49994719498219276\n",
      "iteration 0 / 1: loss 0.4999348956652547\n",
      "iteration 0 / 1: loss 0.4999347889555211\n",
      "iteration 0 / 1: loss 0.4999530899683056\n",
      "iteration 0 / 1: loss 0.49992898984480266\n",
      "iteration 0 / 1: loss 0.4999469233263612\n",
      "iteration 0 / 1: loss 0.49993707202972315\n",
      "iteration 0 / 1: loss 0.49994098383738883\n",
      "iteration 0 / 1: loss 0.49994131001913816\n",
      "iteration 0 / 1: loss 0.49994427235163275\n",
      "iteration 0 / 1: loss 0.4999405694676895\n",
      "iteration 0 / 1: loss 0.49994946197034346\n",
      "iteration 0 / 1: loss 0.4999290008486574\n",
      "iteration 0 / 1: loss 0.4999427682611025\n",
      "iteration 0 / 1: loss 0.4999400616067806\n",
      "iteration 0 / 1: loss 0.49991611245104517\n",
      "iteration 0 / 1: loss 0.4999333634669644\n",
      "iteration 0 / 1: loss 0.4999273686550816\n",
      "iteration 0 / 1: loss 0.4999201352384962\n",
      "iteration 0 / 1: loss 0.4999452501401528\n",
      "iteration 0 / 1: loss 0.49995237887970995\n",
      "iteration 0 / 1: loss 0.49992730296496746\n",
      "iteration 0 / 1: loss 0.4999352153715125\n",
      "iteration 0 / 1: loss 0.49994442447417303\n",
      "iteration 0 / 1: loss 0.49992721237082033\n",
      "iteration 0 / 1: loss 0.4999304134984847\n",
      "iteration 0 / 1: loss 0.49994379025170116\n",
      "iteration 0 / 1: loss 0.49993642356171697\n",
      "iteration 0 / 1: loss 0.49994114950486357\n",
      "iteration 0 / 1: loss 0.4999193393195519\n",
      "iteration 0 / 1: loss 0.4999338356718895\n",
      "iteration 0 / 1: loss 0.49993765801445\n",
      "iteration 0 / 1: loss 0.4999265058381582\n",
      "iteration 0 / 1: loss 0.4999392476682811\n",
      "iteration 0 / 1: loss 0.49992681170567027\n",
      "iteration 0 / 1: loss 0.49994731416072213\n",
      "iteration 0 / 1: loss 0.4999212229057177\n",
      "iteration 0 / 1: loss 0.4999372513601725\n",
      "iteration 0 / 1: loss 0.49994821300501485\n",
      "iteration 0 / 1: loss 0.4999513594035379\n",
      "iteration 0 / 1: loss 0.4999368019217653\n",
      "iteration 0 / 1: loss 0.49992631856719544\n",
      "iteration 0 / 1: loss 0.49995012329537564\n",
      "iteration 0 / 1: loss 0.4999415420617946\n",
      "iteration 0 / 1: loss 0.49994771644127867\n",
      "iteration 0 / 1: loss 0.499934389853526\n",
      "iteration 0 / 1: loss 0.49992934818503487\n",
      "iteration 0 / 1: loss 0.49994062776890213\n",
      "iteration 0 / 1: loss 0.4999260617296278\n",
      "iteration 0 / 1: loss 0.499929538092286\n",
      "iteration 0 / 101: loss 0.4998756481249997\n",
      "iteration 100 / 101: loss 0.5000283877794712\n",
      "iteration 0 / 101: loss 0.4999454540372839\n",
      "iteration 100 / 101: loss 0.49966591093516327\n",
      "iteration 0 / 101: loss 0.500083580545603\n",
      "iteration 100 / 101: loss 0.4999729103545162\n",
      "iteration 0 / 101: loss 0.499660728891419\n",
      "iteration 100 / 101: loss 0.5001337700444365\n",
      "iteration 0 / 101: loss 0.49984689004592886\n",
      "iteration 100 / 101: loss 0.4998921351179353\n",
      "iteration 0 / 101: loss 0.4998344397861626\n",
      "iteration 100 / 101: loss 0.49968861364654005\n",
      "iteration 0 / 101: loss 0.5000568780422555\n",
      "iteration 100 / 101: loss 0.4998224139924277\n",
      "iteration 0 / 101: loss 0.5000137136948589\n",
      "iteration 100 / 101: loss 0.49984984531997645\n",
      "iteration 0 / 101: loss 0.5001955532722733\n",
      "iteration 100 / 101: loss 0.49955083038395653\n",
      "iteration 0 / 101: loss 0.4999551991884554\n",
      "iteration 100 / 101: loss 0.4997423207837515\n",
      "iteration 0 / 101: loss 0.49979963668409294\n",
      "iteration 100 / 101: loss 0.4999156991682902\n",
      "iteration 0 / 101: loss 0.4998173466802416\n",
      "iteration 100 / 101: loss 0.4998844614681858\n",
      "iteration 0 / 101: loss 0.4999207876228205\n",
      "iteration 100 / 101: loss 0.4997945939485236\n",
      "iteration 0 / 101: loss 0.5001437536451779\n",
      "iteration 100 / 101: loss 0.49988232288748774\n",
      "iteration 0 / 101: loss 0.4996531128609713\n",
      "iteration 100 / 101: loss 0.5001463618619936\n",
      "iteration 0 / 101: loss 0.4998012145337895\n",
      "iteration 100 / 101: loss 0.4999250524648765\n",
      "iteration 0 / 101: loss 0.500033965912304\n",
      "iteration 100 / 101: loss 0.4998692249212403\n",
      "iteration 0 / 101: loss 0.49991527394235363\n",
      "iteration 100 / 101: loss 0.49976700523173706\n",
      "iteration 0 / 101: loss 0.5000400567456201\n",
      "iteration 100 / 101: loss 0.49975284540380505\n",
      "iteration 0 / 101: loss 0.4999784119383141\n",
      "iteration 100 / 101: loss 0.5000564970300273\n",
      "iteration 0 / 101: loss 0.49963200296764665\n",
      "iteration 100 / 101: loss 0.49970359628338956\n",
      "iteration 0 / 101: loss 0.4998199450552953\n",
      "iteration 100 / 101: loss 0.5002192472428738\n",
      "iteration 0 / 101: loss 0.5000933420372309\n",
      "iteration 100 / 101: loss 0.49986523647285974\n",
      "iteration 0 / 101: loss 0.4996889765854533\n",
      "iteration 100 / 101: loss 0.5001882826170124\n",
      "iteration 0 / 101: loss 0.4998476506302846\n",
      "iteration 100 / 101: loss 0.5000037456693375\n",
      "iteration 0 / 101: loss 0.4994405005548524\n",
      "iteration 100 / 101: loss 0.4997007621033827\n",
      "iteration 0 / 101: loss 0.4997189594671476\n",
      "iteration 100 / 101: loss 0.4999046153589234\n",
      "iteration 0 / 101: loss 0.5000776018567089\n",
      "iteration 100 / 101: loss 0.49982083603690775\n",
      "iteration 0 / 101: loss 0.4999353102995451\n",
      "iteration 100 / 101: loss 0.5000523273593162\n",
      "iteration 0 / 101: loss 0.4999516265619837\n",
      "iteration 100 / 101: loss 0.4998607758999078\n",
      "iteration 0 / 101: loss 0.4999211160290334\n",
      "iteration 100 / 101: loss 0.49988179274003947\n",
      "iteration 0 / 101: loss 0.4996285824111453\n",
      "iteration 100 / 101: loss 0.5001277442732972\n",
      "iteration 0 / 101: loss 0.4993847312715544\n",
      "iteration 100 / 101: loss 0.499296382989061\n",
      "iteration 0 / 101: loss 0.49979209089376603\n",
      "iteration 100 / 101: loss 0.49954944045516997\n",
      "iteration 0 / 101: loss 0.5000467591543095\n",
      "iteration 100 / 101: loss 0.49974223393176254\n",
      "iteration 0 / 101: loss 0.5000965329150362\n",
      "iteration 100 / 101: loss 0.5000180474350844\n",
      "iteration 0 / 101: loss 0.4998075565892511\n",
      "iteration 100 / 101: loss 0.4994346463995427\n",
      "iteration 0 / 101: loss 0.49989861748276093\n",
      "iteration 100 / 101: loss 0.49946230310149203\n",
      "iteration 0 / 101: loss 0.4992802706106586\n",
      "iteration 100 / 101: loss 0.49992224227611815\n",
      "iteration 0 / 101: loss 0.4999225124284838\n",
      "iteration 100 / 101: loss 0.4998148077838817\n",
      "iteration 0 / 101: loss 0.4994678098389196\n",
      "iteration 100 / 101: loss 0.4994199502454189\n",
      "iteration 0 / 101: loss 0.4996971104054104\n",
      "iteration 100 / 101: loss 0.5000233772764487\n",
      "iteration 0 / 101: loss 0.4994709887448522\n",
      "iteration 100 / 101: loss 0.49978165581459544\n",
      "iteration 0 / 101: loss 0.5000669075113211\n",
      "iteration 100 / 101: loss 0.499409105408225\n",
      "iteration 0 / 101: loss 0.49990767117733204\n",
      "iteration 100 / 101: loss 0.4996836017755881\n",
      "iteration 0 / 101: loss 0.49970321286725244\n",
      "iteration 100 / 101: loss 0.4995046805847154\n",
      "iteration 0 / 101: loss 0.4998852609518685\n",
      "iteration 100 / 101: loss 0.49976376507997855\n",
      "iteration 0 / 101: loss 0.4998338969238881\n",
      "iteration 100 / 101: loss 0.4998262916063733\n",
      "iteration 0 / 101: loss 0.4994263846068525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 101: loss 0.499454334173158\n",
      "iteration 0 / 101: loss 0.49953954941021195\n",
      "iteration 100 / 101: loss 0.5000366265257359\n",
      "iteration 0 / 101: loss 0.4994253990389354\n",
      "iteration 100 / 101: loss 0.4999850675981922\n",
      "iteration 0 / 101: loss 0.4994759696459499\n",
      "iteration 100 / 101: loss 0.49982802258030634\n",
      "iteration 0 / 101: loss 0.4990690386160562\n",
      "iteration 100 / 101: loss 0.49966299105325973\n",
      "iteration 0 / 101: loss 0.4998432792161009\n",
      "iteration 100 / 101: loss 0.4998112059333125\n",
      "iteration 0 / 101: loss 0.4991600336289611\n",
      "iteration 100 / 101: loss 0.4997312067390297\n",
      "iteration 0 / 101: loss 0.4993992644375204\n",
      "iteration 100 / 101: loss 0.4997145905881063\n",
      "iteration 0 / 101: loss 0.4995985303676794\n",
      "iteration 100 / 101: loss 0.4993037407853294\n",
      "iteration 0 / 101: loss 0.4998478176694036\n",
      "iteration 100 / 101: loss 0.49881037224962504\n",
      "iteration 0 / 101: loss 0.49936653980001944\n",
      "iteration 100 / 101: loss 0.49980544914330377\n",
      "iteration 0 / 101: loss 0.499891065598008\n",
      "iteration 100 / 101: loss 0.49969567634991996\n",
      "iteration 0 / 101: loss 0.49977017050601047\n",
      "iteration 100 / 101: loss 0.4997641552125635\n",
      "iteration 0 / 101: loss 0.4997483853005948\n",
      "iteration 100 / 101: loss 0.4991003169342509\n",
      "iteration 0 / 101: loss 0.4993696196599826\n",
      "iteration 100 / 101: loss 0.4996155038290739\n",
      "iteration 0 / 101: loss 0.49974170231463555\n",
      "iteration 100 / 101: loss 0.49915486614347715\n",
      "iteration 0 / 101: loss 0.49954232758741723\n",
      "iteration 100 / 101: loss 0.49957893631386124\n",
      "iteration 0 / 101: loss 0.49925050520568137\n",
      "iteration 100 / 101: loss 0.49973060106498607\n",
      "iteration 0 / 101: loss 0.4996232973258977\n",
      "iteration 100 / 101: loss 0.4997400606098771\n",
      "iteration 0 / 101: loss 0.4992168769794638\n",
      "iteration 100 / 101: loss 0.49939777699008847\n",
      "iteration 0 / 101: loss 0.4997144864827742\n",
      "iteration 100 / 101: loss 0.49869109192469796\n",
      "iteration 0 / 101: loss 0.4996177995583644\n",
      "iteration 100 / 101: loss 0.4993487427095384\n",
      "iteration 0 / 101: loss 0.49934737696002424\n",
      "iteration 100 / 101: loss 0.4997787579841572\n",
      "iteration 0 / 101: loss 0.4998735154846002\n",
      "iteration 100 / 101: loss 0.49909441340658545\n",
      "iteration 0 / 101: loss 0.499292495500291\n",
      "iteration 100 / 101: loss 0.49973770514452687\n",
      "iteration 0 / 101: loss 0.4998015036338051\n",
      "iteration 100 / 101: loss 0.49930894419561644\n",
      "iteration 0 / 101: loss 0.4995004224743674\n",
      "iteration 100 / 101: loss 0.49884927800898704\n",
      "iteration 0 / 101: loss 0.49949858554760296\n",
      "iteration 100 / 101: loss 0.4990359880245822\n",
      "iteration 0 / 101: loss 0.4994391295177617\n",
      "iteration 100 / 101: loss 0.4983209966146669\n",
      "iteration 0 / 101: loss 0.49952957729315395\n",
      "iteration 100 / 101: loss 0.49943573850521117\n",
      "iteration 0 / 101: loss 0.499513402046103\n",
      "iteration 100 / 101: loss 0.4993820079342123\n",
      "iteration 0 / 101: loss 0.49924928821702064\n",
      "iteration 100 / 101: loss 0.49930480942902655\n",
      "iteration 0 / 101: loss 0.4993310584910235\n",
      "iteration 100 / 101: loss 0.49835632083773534\n",
      "iteration 0 / 101: loss 0.49922705761850106\n",
      "iteration 100 / 101: loss 0.49957631128952673\n",
      "iteration 0 / 101: loss 0.499612570895583\n",
      "iteration 100 / 101: loss 0.49927727197019983\n",
      "iteration 0 / 101: loss 0.49903774861306643\n",
      "iteration 100 / 101: loss 0.4995650570405427\n",
      "iteration 0 / 101: loss 0.4989235283448378\n",
      "iteration 100 / 101: loss 0.49959416469234263\n",
      "iteration 0 / 101: loss 0.49907041724305895\n",
      "iteration 100 / 101: loss 0.4994285348942473\n",
      "iteration 0 / 101: loss 0.49934864183211847\n",
      "iteration 100 / 101: loss 0.49902911700944863\n",
      "iteration 0 / 101: loss 0.4992767140353927\n",
      "iteration 100 / 101: loss 0.49938752177157275\n",
      "iteration 0 / 101: loss 0.4980297515875708\n",
      "iteration 100 / 101: loss 0.4991848667878935\n",
      "iteration 0 / 101: loss 0.49912781901850084\n",
      "iteration 100 / 101: loss 0.4982057189005557\n",
      "iteration 0 / 101: loss 0.4989929430222129\n",
      "iteration 100 / 101: loss 0.49898448042698407\n",
      "iteration 0 / 101: loss 0.49896700205935474\n",
      "iteration 100 / 101: loss 0.49905545408861646\n",
      "iteration 0 / 101: loss 0.49900751900681395\n",
      "iteration 100 / 101: loss 0.4989384293876907\n",
      "iteration 0 / 101: loss 0.4989264083043778\n",
      "iteration 100 / 101: loss 0.49877840238458476\n",
      "iteration 0 / 101: loss 0.49872147024758773\n",
      "iteration 100 / 101: loss 0.498990054622288\n",
      "iteration 0 / 101: loss 0.49905415013838167\n",
      "iteration 100 / 101: loss 0.4990533286354009\n",
      "iteration 0 / 101: loss 0.4988103029905964\n",
      "iteration 100 / 101: loss 0.49881916702446005\n",
      "iteration 0 / 101: loss 0.4988302018760597\n",
      "iteration 100 / 101: loss 0.4989367911269286\n",
      "iteration 0 / 101: loss 0.4989100358659812\n",
      "iteration 100 / 101: loss 0.49877171027540274\n",
      "iteration 0 / 101: loss 0.4988876873202371\n",
      "iteration 100 / 101: loss 0.49896336311760275\n",
      "iteration 0 / 101: loss 0.4989381780630653\n",
      "iteration 100 / 101: loss 0.49875105879056325\n",
      "iteration 0 / 101: loss 0.4989015846099084\n",
      "iteration 100 / 101: loss 0.49877506051238346\n",
      "iteration 0 / 101: loss 0.49894585688275683\n",
      "iteration 100 / 101: loss 0.4988688023753301\n",
      "iteration 0 / 101: loss 0.4988616219406764\n",
      "iteration 100 / 101: loss 0.4989343384497604\n",
      "iteration 0 / 101: loss 0.4987234678768956\n",
      "iteration 100 / 101: loss 0.498840741632628\n",
      "iteration 0 / 101: loss 0.49885193862605864\n",
      "iteration 100 / 101: loss 0.4989742660459103\n",
      "iteration 0 / 101: loss 0.4988024239682324\n",
      "iteration 100 / 101: loss 0.4988095599073786\n",
      "iteration 0 / 101: loss 0.4988552511343461\n",
      "iteration 100 / 101: loss 0.49886266779876537\n",
      "iteration 0 / 101: loss 0.4989269930287089\n",
      "iteration 100 / 101: loss 0.4988798466373651\n",
      "iteration 0 / 101: loss 0.49869459276375405\n",
      "iteration 100 / 101: loss 0.49893468285107395\n",
      "iteration 0 / 101: loss 0.49874637105257985\n",
      "iteration 100 / 101: loss 0.4987776378640786\n",
      "iteration 0 / 101: loss 0.498637238408699\n",
      "iteration 100 / 101: loss 0.49901823580235866\n",
      "iteration 0 / 101: loss 0.49883611326172217\n",
      "iteration 100 / 101: loss 0.4987821037628931\n",
      "iteration 0 / 101: loss 0.49880993053281775\n",
      "iteration 100 / 101: loss 0.49879412220819125\n",
      "iteration 0 / 101: loss 0.49870597347421414\n",
      "iteration 100 / 101: loss 0.49877828104200406\n",
      "iteration 0 / 101: loss 0.4986447244615345\n",
      "iteration 100 / 101: loss 0.4987887799800477\n",
      "iteration 0 / 101: loss 0.49871582549656485\n",
      "iteration 100 / 101: loss 0.49883354862865603\n",
      "iteration 0 / 101: loss 0.49886383581291643\n",
      "iteration 100 / 101: loss 0.49880471535793197\n",
      "iteration 0 / 101: loss 0.4987060570349339\n",
      "iteration 100 / 101: loss 0.4986020176992631\n",
      "iteration 0 / 101: loss 0.49885049278920957\n",
      "iteration 100 / 101: loss 0.4987901278270644\n",
      "iteration 0 / 101: loss 0.4988983677244649\n",
      "iteration 100 / 101: loss 0.49864238062919813\n",
      "iteration 0 / 101: loss 0.4986071052645164\n",
      "iteration 100 / 101: loss 0.498832991354207\n",
      "iteration 0 / 101: loss 0.49876395143386537\n",
      "iteration 100 / 101: loss 0.49879309624254564\n",
      "iteration 0 / 101: loss 0.49850999260769574\n",
      "iteration 100 / 101: loss 0.4986746783267779\n",
      "iteration 0 / 101: loss 0.49860497346417876\n",
      "iteration 100 / 101: loss 0.49858015241915\n",
      "iteration 0 / 101: loss 0.49879725481946036\n",
      "iteration 100 / 101: loss 0.4986248465647998\n",
      "iteration 0 / 101: loss 0.4986155248982882\n",
      "iteration 100 / 101: loss 0.49889283083890734\n",
      "iteration 0 / 101: loss 0.4986601950294466\n",
      "iteration 100 / 101: loss 0.49872865230026175\n",
      "iteration 0 / 101: loss 0.4985684522271876\n",
      "iteration 100 / 101: loss 0.49868072130561997\n",
      "iteration 0 / 101: loss 0.49866179181981046\n",
      "iteration 100 / 101: loss 0.4985740101859309\n",
      "iteration 0 / 101: loss 0.4987231255796531\n",
      "iteration 100 / 101: loss 0.49853089774116127\n",
      "iteration 0 / 101: loss 0.49874360061156203\n",
      "iteration 100 / 101: loss 0.4983998306236708\n",
      "iteration 0 / 101: loss 0.49858402223409926\n",
      "iteration 100 / 101: loss 0.4985099305540393\n",
      "iteration 0 / 101: loss 0.49859817599370804\n",
      "iteration 100 / 101: loss 0.49852496946506947\n",
      "iteration 0 / 101: loss 0.4985787165884237\n",
      "iteration 100 / 101: loss 0.4984872860266785\n",
      "iteration 0 / 101: loss 0.49852523956052813\n",
      "iteration 100 / 101: loss 0.4986800357831199\n",
      "iteration 0 / 101: loss 0.49843195121760764\n",
      "iteration 100 / 101: loss 0.4985583989101481\n",
      "iteration 0 / 101: loss 0.49837024132641705\n",
      "iteration 100 / 101: loss 0.498294984840956\n",
      "iteration 0 / 101: loss 0.4983169075700872\n",
      "iteration 100 / 101: loss 0.498429338541385\n",
      "iteration 0 / 101: loss 0.4982320584582125\n",
      "iteration 100 / 101: loss 0.498343849131937\n",
      "iteration 0 / 101: loss 0.4984458192829068\n",
      "iteration 100 / 101: loss 0.49821895419141565\n",
      "iteration 0 / 101: loss 0.4984359247801663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 101: loss 0.498482516343212\n",
      "iteration 0 / 101: loss 0.49835961490071584\n",
      "iteration 100 / 101: loss 0.49850115135954465\n",
      "iteration 0 / 101: loss 0.49821715165479585\n",
      "iteration 100 / 101: loss 0.49824007950673\n",
      "iteration 0 / 101: loss 0.4981042897620912\n",
      "iteration 100 / 101: loss 0.49836462592454717\n",
      "iteration 0 / 101: loss 0.4981650085048224\n",
      "iteration 100 / 101: loss 0.4984363005232169\n",
      "iteration 0 / 101: loss 0.49825119299399273\n",
      "iteration 100 / 101: loss 0.498277684373536\n",
      "iteration 0 / 101: loss 0.49809774931207595\n",
      "iteration 100 / 101: loss 0.4978384415345347\n",
      "iteration 0 / 101: loss 0.49824235469843664\n",
      "iteration 100 / 101: loss 0.4980856827076539\n",
      "iteration 0 / 101: loss 0.4980769217056019\n",
      "iteration 100 / 101: loss 0.4980259342907545\n",
      "iteration 0 / 101: loss 0.4980803571972914\n",
      "iteration 100 / 101: loss 0.49792388860470843\n",
      "iteration 0 / 101: loss 0.4979387428056199\n",
      "iteration 100 / 101: loss 0.49790150224668894\n",
      "iteration 0 / 101: loss 0.4980166659151554\n",
      "iteration 100 / 101: loss 0.4978934283583049\n",
      "iteration 0 / 101: loss 0.4978471077897114\n",
      "iteration 100 / 101: loss 0.49815965508662274\n",
      "iteration 0 / 101: loss 0.4977204800325771\n",
      "iteration 100 / 101: loss 0.4978371503948019\n",
      "iteration 0 / 101: loss 0.4983275177265637\n",
      "iteration 100 / 101: loss 0.4978263159946013\n",
      "iteration 0 / 101: loss 0.4980115387362272\n",
      "iteration 100 / 101: loss 0.4975097168701757\n",
      "iteration 0 / 101: loss 0.4978466691062383\n",
      "iteration 100 / 101: loss 0.4977618229606677\n",
      "iteration 0 / 101: loss 0.4975396055529599\n",
      "iteration 100 / 101: loss 0.49796789334697167\n",
      "iteration 0 / 101: loss 0.49758244687017783\n",
      "iteration 100 / 101: loss 0.497518540781918\n",
      "iteration 0 / 101: loss 0.4976723043097368\n",
      "iteration 100 / 101: loss 0.49760796717644273\n",
      "iteration 0 / 101: loss 0.49771687255270686\n",
      "iteration 100 / 101: loss 0.4970950620875421\n",
      "iteration 0 / 101: loss 0.4973732313110889\n",
      "iteration 100 / 101: loss 0.49739386867265123\n",
      "iteration 0 / 101: loss 0.4973123596314996\n",
      "iteration 100 / 101: loss 0.4976358672284656\n",
      "iteration 0 / 101: loss 0.49772606511353296\n",
      "iteration 100 / 101: loss 0.4973418067164781\n",
      "iteration 0 / 101: loss 0.49732025653945083\n",
      "iteration 100 / 101: loss 0.4976964309505597\n",
      "iteration 0 / 101: loss 0.49722712401857183\n",
      "iteration 100 / 101: loss 0.4969754891453621\n",
      "iteration 0 / 101: loss 0.49715081832166363\n",
      "iteration 100 / 101: loss 0.4969973628976347\n",
      "iteration 0 / 101: loss 0.4971741517196673\n",
      "iteration 100 / 101: loss 0.4974393253638024\n",
      "iteration 0 / 101: loss 0.4970285406637158\n",
      "iteration 100 / 101: loss 0.49671022786146257\n",
      "iteration 0 / 101: loss 0.49687562035012\n",
      "iteration 100 / 101: loss 0.496870572183581\n",
      "iteration 0 / 101: loss 0.49706129123351883\n",
      "iteration 100 / 101: loss 0.4965930923342558\n",
      "iteration 0 / 101: loss 0.49730751704100745\n",
      "iteration 100 / 101: loss 0.4968851825277986\n",
      "iteration 0 / 101: loss 0.4966062919681437\n",
      "iteration 100 / 101: loss 0.4967628829862315\n",
      "iteration 0 / 101: loss 0.4966867041843735\n",
      "iteration 100 / 101: loss 0.4968065012002698\n",
      "iteration 0 / 101: loss 0.4969118414924669\n",
      "iteration 100 / 101: loss 0.49688064108097457\n",
      "iteration 0 / 101: loss 0.4963770598813772\n",
      "iteration 100 / 101: loss 0.49620496734185887\n",
      "iteration 0 / 101: loss 0.4957691235338782\n",
      "iteration 100 / 101: loss 0.49614480254845644\n",
      "iteration 0 / 101: loss 0.4959715257955059\n",
      "iteration 100 / 101: loss 0.49579574991812914\n",
      "iteration 0 / 101: loss 0.49574778159472555\n",
      "iteration 100 / 101: loss 0.4959124803767452\n",
      "iteration 0 / 101: loss 0.49668262692688575\n",
      "iteration 100 / 101: loss 0.49600472151455455\n",
      "iteration 0 / 101: loss 0.4956603479079064\n",
      "iteration 100 / 101: loss 0.4960113709988356\n",
      "iteration 0 / 101: loss 0.49595753490300554\n",
      "iteration 100 / 101: loss 0.4956303210826078\n",
      "iteration 0 / 101: loss 0.49592364890649887\n",
      "iteration 100 / 101: loss 0.49624683742944037\n",
      "iteration 0 / 101: loss 0.4952031536551766\n",
      "iteration 100 / 101: loss 0.4958138360093452\n",
      "iteration 0 / 101: loss 0.4963454735120433\n",
      "iteration 100 / 101: loss 0.49589798938972335\n",
      "iteration 0 / 101: loss 0.49575405196635314\n",
      "iteration 100 / 101: loss 0.4952378097876552\n",
      "iteration 0 / 101: loss 0.49584434054905296\n",
      "iteration 100 / 101: loss 0.49577989153367075\n",
      "iteration 0 / 101: loss 0.49619015556030305\n",
      "iteration 100 / 101: loss 0.4954099692301195\n",
      "iteration 0 / 101: loss 0.4963738400997528\n",
      "iteration 100 / 101: loss 0.49628890822935245\n",
      "iteration 0 / 101: loss 0.495693155173699\n",
      "iteration 100 / 101: loss 0.4955169392925631\n",
      "iteration 0 / 101: loss 0.49595948474811147\n",
      "iteration 100 / 101: loss 0.49578373593959707\n",
      "iteration 0 / 101: loss 0.49562997930677605\n",
      "iteration 100 / 101: loss 0.49573487139567857\n",
      "iteration 0 / 101: loss 0.49569022603916846\n",
      "iteration 100 / 101: loss 0.4957704311887677\n",
      "iteration 0 / 101: loss 0.49612094985573224\n",
      "iteration 100 / 101: loss 0.4965470811477582\n",
      "iteration 0 / 101: loss 0.49626297301606775\n",
      "iteration 100 / 101: loss 0.495598874297337\n",
      "iteration 0 / 101: loss 0.4957534904485279\n",
      "iteration 100 / 101: loss 0.49559781996963276\n",
      "iteration 0 / 101: loss 0.4955652512589917\n",
      "iteration 100 / 101: loss 0.49579556288148363\n",
      "iteration 0 / 101: loss 0.4959958112337259\n",
      "iteration 100 / 101: loss 0.4952609514243297\n",
      "iteration 0 / 101: loss 0.4956375794957239\n",
      "iteration 100 / 101: loss 0.49503432236497474\n",
      "iteration 0 / 101: loss 0.49563034171441345\n",
      "iteration 100 / 101: loss 0.4954729807606897\n",
      "iteration 0 / 101: loss 0.4956671184222693\n",
      "iteration 100 / 101: loss 0.4954221644499609\n",
      "iteration 0 / 101: loss 0.49557045098827573\n",
      "iteration 100 / 101: loss 0.49527027767812276\n",
      "iteration 0 / 101: loss 0.4956603303475057\n",
      "iteration 100 / 101: loss 0.49581249317048715\n",
      "iteration 0 / 101: loss 0.49517534058056945\n",
      "iteration 100 / 101: loss 0.495443526698828\n",
      "iteration 0 / 101: loss 0.4953310583159128\n",
      "iteration 100 / 101: loss 0.4952038521752485\n",
      "iteration 0 / 101: loss 0.4951118997339075\n",
      "iteration 100 / 101: loss 0.4951171866630432\n",
      "iteration 0 / 101: loss 0.4954539916579014\n",
      "iteration 100 / 101: loss 0.4950806257544641\n",
      "iteration 0 / 101: loss 0.49565451977623437\n",
      "iteration 100 / 101: loss 0.49503672134688315\n",
      "iteration 0 / 101: loss 0.4950148358093248\n",
      "iteration 100 / 101: loss 0.49499862068172495\n",
      "iteration 0 / 101: loss 0.49493910435928773\n",
      "iteration 100 / 101: loss 0.4950922597855208\n",
      "iteration 0 / 101: loss 0.4945548594950179\n",
      "iteration 100 / 101: loss 0.49477152017988324\n",
      "iteration 0 / 101: loss 0.4946474036004214\n",
      "iteration 100 / 101: loss 0.49485520110622777\n",
      "iteration 0 / 101: loss 0.49496936862123153\n",
      "iteration 100 / 101: loss 0.49471226952867353\n",
      "iteration 0 / 101: loss 0.4952435250702973\n",
      "iteration 100 / 101: loss 0.4946074144999141\n",
      "iteration 0 / 101: loss 0.49448467626562775\n",
      "iteration 100 / 101: loss 0.49588825822829097\n",
      "iteration 0 / 101: loss 0.4951536084035401\n",
      "iteration 100 / 101: loss 0.49433290077648884\n",
      "iteration 0 / 101: loss 0.4935836981837409\n",
      "iteration 100 / 101: loss 0.4940522388701971\n",
      "iteration 0 / 101: loss 0.4946089437057003\n",
      "iteration 100 / 101: loss 0.4947827911455172\n",
      "iteration 0 / 101: loss 0.49458481496607337\n",
      "iteration 100 / 101: loss 0.4946886937676197\n",
      "iteration 0 / 101: loss 0.4944606029891584\n",
      "iteration 100 / 101: loss 0.4937648895775486\n",
      "iteration 0 / 101: loss 0.49383411612371214\n",
      "iteration 100 / 101: loss 0.4945499406182881\n",
      "iteration 0 / 101: loss 0.4949080164741433\n",
      "iteration 100 / 101: loss 0.4940101835165985\n",
      "iteration 0 / 101: loss 0.49430871742974997\n",
      "iteration 100 / 101: loss 0.4938013179128331\n",
      "iteration 0 / 101: loss 0.4948753255378558\n",
      "iteration 100 / 101: loss 0.4939914200286974\n",
      "iteration 0 / 101: loss 0.4937839192828349\n",
      "iteration 100 / 101: loss 0.49321089820357256\n",
      "iteration 0 / 101: loss 0.49383721756275173\n",
      "iteration 100 / 101: loss 0.4935428270808935\n",
      "iteration 0 / 101: loss 0.49405949842951\n",
      "iteration 100 / 101: loss 0.4942054556373693\n",
      "iteration 0 / 101: loss 0.49336483225657435\n",
      "iteration 100 / 101: loss 0.49359484056741043\n",
      "iteration 0 / 101: loss 0.49416654538279314\n",
      "iteration 100 / 101: loss 0.49405572601087805\n",
      "iteration 0 / 101: loss 0.4927651044549738\n",
      "iteration 100 / 101: loss 0.4942362164483884\n",
      "iteration 0 / 101: loss 0.4934611199134649\n",
      "iteration 100 / 101: loss 0.4932216172221872\n",
      "iteration 0 / 101: loss 0.493475363879385\n",
      "iteration 100 / 101: loss 0.4925724601620655\n",
      "iteration 0 / 101: loss 0.49333174733991125\n",
      "iteration 100 / 101: loss 0.4934612611850401\n",
      "iteration 0 / 101: loss 0.4931743715895483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 101: loss 0.4934929415858342\n",
      "iteration 0 / 101: loss 0.493430061674062\n",
      "iteration 100 / 101: loss 0.4938018193627136\n",
      "iteration 0 / 101: loss 0.4929420883045639\n",
      "iteration 100 / 101: loss 0.4938230292837607\n",
      "iteration 0 / 101: loss 0.4924970285218191\n",
      "iteration 100 / 101: loss 0.49248182045530947\n",
      "iteration 0 / 101: loss 0.4920004697415367\n",
      "iteration 100 / 101: loss 0.4921993610885574\n",
      "iteration 0 / 101: loss 0.49293975832310877\n",
      "iteration 100 / 101: loss 0.4917456649903819\n",
      "iteration 0 / 101: loss 0.4924817277700197\n",
      "iteration 100 / 101: loss 0.4923978932672128\n",
      "iteration 0 / 101: loss 0.4921187189772114\n",
      "iteration 100 / 101: loss 0.4915057776834061\n",
      "iteration 0 / 101: loss 0.4930245541728921\n",
      "iteration 100 / 101: loss 0.49171878451237433\n",
      "iteration 0 / 101: loss 0.4918831171117226\n",
      "iteration 100 / 101: loss 0.49188140090773025\n",
      "iteration 0 / 101: loss 0.49241088363841523\n",
      "iteration 100 / 101: loss 0.49105391620289846\n",
      "iteration 0 / 101: loss 0.49141884748953074\n",
      "iteration 100 / 101: loss 0.49066602342845506\n",
      "iteration 0 / 101: loss 0.49132847207153957\n",
      "iteration 100 / 101: loss 0.4904945193071602\n",
      "iteration 0 / 101: loss 0.49175007893083633\n",
      "iteration 100 / 101: loss 0.49001081301500293\n",
      "iteration 0 / 101: loss 0.49077554822832975\n",
      "iteration 100 / 101: loss 0.48998601001612574\n",
      "iteration 0 / 101: loss 0.4910319596261529\n",
      "iteration 100 / 101: loss 0.49093892951649865\n",
      "iteration 0 / 101: loss 0.4907571640679913\n",
      "iteration 100 / 101: loss 0.49066802048952796\n",
      "iteration 0 / 101: loss 0.489842449732558\n",
      "iteration 100 / 101: loss 0.49015475660053687\n",
      "iteration 0 / 101: loss 0.4903236161840155\n",
      "iteration 100 / 101: loss 0.4892302088526239\n",
      "iteration 0 / 101: loss 0.4904667019967371\n",
      "iteration 100 / 101: loss 0.4896968614770034\n",
      "iteration 0 / 101: loss 0.49082150033699146\n",
      "iteration 100 / 101: loss 0.4896594109221858\n",
      "iteration 0 / 101: loss 0.4877978247771491\n",
      "iteration 100 / 101: loss 0.48822517819351674\n",
      "iteration 0 / 101: loss 0.48941010563073006\n",
      "iteration 100 / 101: loss 0.4898541279066404\n",
      "iteration 0 / 101: loss 0.48922327662277354\n",
      "iteration 100 / 101: loss 0.4885919466032631\n",
      "iteration 0 / 101: loss 0.488035007210148\n",
      "iteration 100 / 101: loss 0.48897302342838495\n",
      "iteration 0 / 101: loss 0.48963061575170574\n",
      "iteration 100 / 101: loss 0.48844709856560975\n",
      "iteration 0 / 101: loss 0.48747903699312833\n",
      "iteration 100 / 101: loss 0.4887278792068746\n",
      "iteration 0 / 101: loss 0.48779737681937413\n",
      "iteration 100 / 101: loss 0.48911238806962526\n",
      "iteration 0 / 101: loss 0.48697573315527204\n",
      "iteration 100 / 101: loss 0.48676257659526145\n",
      "iteration 0 / 101: loss 0.48741811211195346\n",
      "iteration 100 / 101: loss 0.48549384352532077\n",
      "iteration 0 / 101: loss 0.48618641192962064\n",
      "iteration 100 / 101: loss 0.4873185159803607\n",
      "iteration 0 / 101: loss 0.4868252361302937\n",
      "iteration 100 / 101: loss 0.48760525326621135\n",
      "iteration 0 / 101: loss 0.4874336695151773\n",
      "iteration 100 / 101: loss 0.4868613507411586\n",
      "iteration 0 / 101: loss 0.4856937308940028\n",
      "iteration 100 / 101: loss 0.48557432739668616\n",
      "iteration 0 / 101: loss 0.48373200839171215\n",
      "iteration 100 / 101: loss 0.4857453725180466\n",
      "iteration 0 / 101: loss 0.48458471871331626\n",
      "iteration 100 / 101: loss 0.4864985496509645\n",
      "iteration 0 / 101: loss 0.48599160536186514\n",
      "iteration 100 / 101: loss 0.485395880280784\n",
      "iteration 0 / 101: loss 0.4845509785327301\n",
      "iteration 100 / 101: loss 0.48423898416681654\n",
      "iteration 0 / 101: loss 0.484613771330976\n",
      "iteration 100 / 101: loss 0.4847512385937586\n",
      "iteration 0 / 101: loss 0.4829687474191649\n",
      "iteration 100 / 101: loss 0.48496055231084656\n",
      "iteration 0 / 101: loss 0.484998690023067\n",
      "iteration 100 / 101: loss 0.4855627573097664\n",
      "iteration 0 / 101: loss 0.48276237666998095\n",
      "iteration 100 / 101: loss 0.48392370034164095\n",
      "iteration 0 / 101: loss 0.48405648145345403\n",
      "iteration 100 / 101: loss 0.48528365079858987\n",
      "iteration 0 / 101: loss 0.48553155889866395\n",
      "iteration 100 / 101: loss 0.48504233608654934\n",
      "iteration 0 / 101: loss 0.48356849981004646\n",
      "iteration 100 / 101: loss 0.48336899721907317\n",
      "iteration 0 / 101: loss 0.48457358165853204\n",
      "iteration 100 / 101: loss 0.48558747247463574\n",
      "iteration 0 / 101: loss 0.48582489428253484\n",
      "iteration 100 / 101: loss 0.4861484027210242\n",
      "iteration 0 / 101: loss 0.48475337515162753\n",
      "iteration 100 / 101: loss 0.4831722045076167\n",
      "iteration 0 / 101: loss 0.4832313598152264\n",
      "iteration 100 / 101: loss 0.4835542472050186\n",
      "iteration 0 / 101: loss 0.48477661389082677\n",
      "iteration 100 / 101: loss 0.4822716051356616\n",
      "iteration 0 / 101: loss 0.4852413003940425\n",
      "iteration 100 / 101: loss 0.48211589356547\n",
      "iteration 0 / 101: loss 0.48564561789874133\n",
      "iteration 100 / 101: loss 0.4835643855713536\n",
      "iteration 0 / 101: loss 0.483369486593371\n",
      "iteration 100 / 101: loss 0.4836971249622578\n",
      "iteration 0 / 101: loss 0.48383528323902597\n",
      "iteration 100 / 101: loss 0.48393258125074473\n",
      "iteration 0 / 101: loss 0.48450905555176754\n",
      "iteration 100 / 101: loss 0.4834484214129934\n",
      "iteration 0 / 101: loss 0.4834941644135303\n",
      "iteration 100 / 101: loss 0.4853061774664282\n",
      "iteration 0 / 101: loss 0.48343070747233363\n",
      "iteration 100 / 101: loss 0.48316731871191293\n",
      "iteration 0 / 101: loss 0.4823891511355566\n",
      "iteration 100 / 101: loss 0.4834983659798572\n",
      "iteration 0 / 101: loss 0.48305042882705834\n",
      "iteration 100 / 101: loss 0.4841454702831506\n",
      "iteration 0 / 101: loss 0.4843208163720169\n",
      "iteration 100 / 101: loss 0.48417670069257585\n",
      "iteration 0 / 101: loss 0.48103676724958333\n",
      "iteration 100 / 101: loss 0.48131662080315785\n",
      "iteration 0 / 101: loss 0.4829904999745879\n",
      "iteration 100 / 101: loss 0.4832273584304537\n",
      "iteration 0 / 101: loss 0.48379963947635046\n",
      "iteration 100 / 101: loss 0.48195385930169476\n",
      "iteration 0 / 101: loss 0.4826691511923493\n",
      "iteration 100 / 101: loss 0.4833589596480283\n",
      "iteration 0 / 101: loss 0.48147132673078646\n",
      "iteration 100 / 101: loss 0.48388180177981843\n",
      "iteration 0 / 101: loss 0.4815084648778944\n",
      "iteration 100 / 101: loss 0.4818571140795722\n",
      "iteration 0 / 101: loss 0.48284415503969386\n",
      "iteration 100 / 101: loss 0.4828800157961038\n",
      "iteration 0 / 101: loss 0.4821660375985132\n",
      "iteration 100 / 101: loss 0.4808954693818456\n",
      "iteration 0 / 101: loss 0.4785825255663841\n",
      "iteration 100 / 101: loss 0.4815980694863937\n",
      "iteration 0 / 101: loss 0.4832647189523739\n",
      "iteration 100 / 101: loss 0.48166016912389426\n",
      "iteration 0 / 101: loss 0.4811352943793929\n",
      "iteration 100 / 101: loss 0.480951373081533\n",
      "iteration 0 / 101: loss 0.48093758138016995\n",
      "iteration 100 / 101: loss 0.48038851111305075\n",
      "iteration 0 / 101: loss 0.4802513665628952\n",
      "iteration 100 / 101: loss 0.48054607038027564\n",
      "iteration 0 / 101: loss 0.4809320708166068\n",
      "iteration 100 / 101: loss 0.48127235846715294\n",
      "iteration 0 / 101: loss 0.4797143796196441\n",
      "iteration 100 / 101: loss 0.48007560390703485\n",
      "iteration 0 / 101: loss 0.47997028233355576\n",
      "iteration 100 / 101: loss 0.4827906039231947\n",
      "iteration 0 / 101: loss 0.47881672132338843\n",
      "iteration 100 / 101: loss 0.48154337267056796\n",
      "iteration 0 / 101: loss 0.4790646645765371\n",
      "iteration 100 / 101: loss 0.4805729594985649\n",
      "iteration 0 / 101: loss 0.47915978454657354\n",
      "iteration 100 / 101: loss 0.480629415036829\n",
      "iteration 0 / 101: loss 0.47866884054139747\n",
      "iteration 100 / 101: loss 0.4809407593098726\n",
      "iteration 0 / 101: loss 0.47851556252622135\n",
      "iteration 100 / 101: loss 0.4782143868217047\n",
      "iteration 0 / 101: loss 0.4777404730089119\n",
      "iteration 100 / 101: loss 0.48015062698197697\n",
      "iteration 0 / 101: loss 0.47620997100863005\n",
      "iteration 100 / 101: loss 0.47987952407967965\n",
      "iteration 0 / 101: loss 0.47930793430553087\n",
      "iteration 100 / 101: loss 0.47930229457451295\n",
      "iteration 0 / 101: loss 0.47565300249889325\n",
      "iteration 100 / 101: loss 0.47817687673708126\n",
      "iteration 0 / 101: loss 0.4764411570626868\n",
      "iteration 100 / 101: loss 0.4769657316032317\n",
      "iteration 0 / 101: loss 0.4781422539297916\n",
      "iteration 100 / 101: loss 0.4762380733687625\n",
      "iteration 0 / 101: loss 0.4761410341029648\n",
      "iteration 100 / 101: loss 0.4788544558356199\n",
      "iteration 0 / 101: loss 0.4786939751436582\n",
      "iteration 100 / 101: loss 0.4777802014512936\n",
      "iteration 0 / 101: loss 0.47666389032858714\n",
      "iteration 100 / 101: loss 0.4741759092846787\n",
      "iteration 0 / 101: loss 0.4795341829671662\n",
      "iteration 100 / 101: loss 0.47992691532003057\n",
      "iteration 0 / 101: loss 0.4777089988166916\n",
      "iteration 100 / 101: loss 0.47648480620435285\n",
      "iteration 0 / 101: loss 0.4789717542502745\n",
      "iteration 100 / 101: loss 0.47705912340114365\n",
      "iteration 0 / 101: loss 0.47362880337631297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 101: loss 0.47379787008109536\n",
      "iteration 0 / 101: loss 0.47439592366609823\n",
      "iteration 100 / 101: loss 0.4761173387121019\n",
      "iteration 0 / 101: loss 0.4769487266484794\n",
      "iteration 100 / 101: loss 0.47309126278604496\n",
      "iteration 0 / 101: loss 0.47549850025602625\n",
      "iteration 100 / 101: loss 0.4778945146217938\n",
      "iteration 0 / 101: loss 0.47506025841858013\n",
      "iteration 100 / 101: loss 0.47477472210436256\n",
      "iteration 0 / 101: loss 0.4764911211503517\n",
      "iteration 100 / 101: loss 0.4754321288129197\n",
      "iteration 0 / 101: loss 0.47373967636280284\n",
      "iteration 100 / 101: loss 0.4720317903083595\n",
      "iteration 0 / 101: loss 0.4727601498284984\n",
      "iteration 100 / 101: loss 0.4726549177150418\n",
      "iteration 0 / 101: loss 0.4716568315650166\n",
      "iteration 100 / 101: loss 0.47528779028376783\n",
      "iteration 0 / 101: loss 0.47326969876437\n",
      "iteration 100 / 101: loss 0.4723720449153624\n",
      "iteration 0 / 101: loss 0.4758790741115158\n",
      "iteration 100 / 101: loss 0.47353392536669986\n",
      "iteration 0 / 101: loss 0.47332422756291753\n",
      "iteration 100 / 101: loss 0.4691813581023044\n",
      "iteration 0 / 101: loss 0.47335446608408904\n",
      "iteration 100 / 101: loss 0.4734794466983484\n",
      "iteration 0 / 101: loss 0.47574524229188886\n",
      "iteration 100 / 101: loss 0.47045077167597293\n",
      "iteration 0 / 101: loss 0.46978217917108145\n",
      "iteration 100 / 101: loss 0.47073541042240635\n",
      "iteration 0 / 101: loss 0.4718291074700056\n",
      "iteration 100 / 101: loss 0.4713225847508894\n",
      "iteration 0 / 101: loss 0.47065495042198086\n",
      "iteration 100 / 101: loss 0.46910686519529227\n",
      "iteration 0 / 101: loss 0.4660122696765646\n",
      "iteration 100 / 101: loss 0.46958469709596623\n",
      "iteration 0 / 101: loss 0.47119738743081296\n",
      "iteration 100 / 101: loss 0.4652479316839639\n",
      "iteration 0 / 101: loss 0.4665409876496642\n",
      "iteration 100 / 101: loss 0.46956249634407704\n",
      "iteration 0 / 101: loss 0.4680542791733129\n",
      "iteration 100 / 101: loss 0.47059696497770065\n",
      "iteration 0 / 101: loss 0.47074203820317356\n",
      "iteration 100 / 101: loss 0.4641691929194119\n",
      "iteration 0 / 101: loss 0.4683066028705958\n",
      "iteration 100 / 101: loss 0.4674084388754539\n",
      "iteration 0 / 101: loss 0.4693345009179705\n",
      "iteration 100 / 101: loss 0.46981267984434427\n",
      "iteration 0 / 101: loss 0.468248320007052\n",
      "iteration 100 / 101: loss 0.4653930813335093\n",
      "iteration 0 / 101: loss 0.4660494642357272\n",
      "iteration 100 / 101: loss 0.46931189577483073\n",
      "iteration 0 / 101: loss 0.46883675212072407\n",
      "iteration 100 / 101: loss 0.46734738734463493\n",
      "iteration 0 / 101: loss 0.46533065031549814\n",
      "iteration 100 / 101: loss 0.4678226123057953\n",
      "iteration 0 / 101: loss 0.46901812981896773\n",
      "iteration 100 / 101: loss 0.4640452182023282\n",
      "iteration 0 / 101: loss 0.4669680612980618\n",
      "iteration 100 / 101: loss 0.4629626850933269\n",
      "iteration 0 / 101: loss 0.4647087256844169\n",
      "iteration 100 / 101: loss 0.4698936885043086\n",
      "iteration 0 / 101: loss 0.46548166874683633\n",
      "iteration 100 / 101: loss 0.4660796113643773\n",
      "iteration 0 / 101: loss 0.4621574861466291\n",
      "iteration 100 / 101: loss 0.4696852320625786\n",
      "iteration 0 / 101: loss 0.4658506910862408\n",
      "iteration 100 / 101: loss 0.46275166866458695\n",
      "iteration 0 / 101: loss 0.4652117649569863\n",
      "iteration 100 / 101: loss 0.46428692781986153\n",
      "iteration 0 / 101: loss 0.4660523960920442\n",
      "iteration 100 / 101: loss 0.4674583605739212\n",
      "iteration 0 / 101: loss 0.46503999725788997\n",
      "iteration 100 / 101: loss 0.4636002851707816\n",
      "iteration 0 / 101: loss 0.461911413608597\n",
      "iteration 100 / 101: loss 0.4657951830116565\n",
      "iteration 0 / 101: loss 0.4666453368318858\n",
      "iteration 100 / 101: loss 0.46516394559467356\n",
      "iteration 0 / 101: loss 0.46964698597364934\n",
      "iteration 100 / 101: loss 0.46345145991995124\n",
      "iteration 0 / 101: loss 0.46298006307126954\n",
      "iteration 100 / 101: loss 0.4657356159551917\n",
      "iteration 0 / 101: loss 0.46119643813356503\n",
      "iteration 100 / 101: loss 0.4663118026349753\n",
      "iteration 0 / 101: loss 0.46756889492901715\n",
      "iteration 100 / 101: loss 0.4650748518996327\n",
      "iteration 0 / 101: loss 0.46546248070200474\n",
      "iteration 100 / 101: loss 0.4650015900245638\n",
      "iteration 0 / 101: loss 0.4675228654973339\n",
      "iteration 100 / 101: loss 0.46282828416792704\n",
      "iteration 0 / 101: loss 0.46272915501253054\n",
      "iteration 100 / 101: loss 0.46500679321134597\n",
      "iteration 0 / 101: loss 0.46348722314342067\n",
      "iteration 100 / 101: loss 0.46623699722250844\n",
      "iteration 0 / 101: loss 0.4597263864696651\n",
      "iteration 100 / 101: loss 0.4678345586392646\n",
      "iteration 0 / 101: loss 0.4648005531981579\n",
      "iteration 100 / 101: loss 0.46226420403428137\n",
      "iteration 0 / 101: loss 0.46534980962155903\n",
      "iteration 100 / 101: loss 0.4598321734481504\n",
      "iteration 0 / 101: loss 0.46548092594623225\n",
      "iteration 100 / 101: loss 0.45973578144689614\n",
      "iteration 0 / 101: loss 0.4670239477532863\n",
      "iteration 100 / 101: loss 0.4685574896839538\n",
      "iteration 0 / 101: loss 0.4688272741304383\n",
      "iteration 100 / 101: loss 0.4653920105473322\n",
      "iteration 0 / 101: loss 0.4656213817834413\n",
      "iteration 100 / 101: loss 0.4671708708769891\n",
      "iteration 0 / 101: loss 0.4619228852928692\n",
      "iteration 100 / 101: loss 0.46464766485521003\n",
      "iteration 0 / 101: loss 0.46366782786286903\n",
      "iteration 100 / 101: loss 0.46181868837710366\n",
      "iteration 0 / 101: loss 0.4632449393891824\n",
      "iteration 100 / 101: loss 0.4677291731882993\n",
      "iteration 0 / 101: loss 0.4654088538449774\n",
      "iteration 100 / 101: loss 0.4612257806638801\n",
      "iteration 0 / 101: loss 0.46678162741854734\n",
      "iteration 100 / 101: loss 0.4631442162783244\n",
      "iteration 0 / 101: loss 0.4566191443138244\n",
      "iteration 100 / 101: loss 0.4626402418721357\n",
      "iteration 0 / 101: loss 0.46494672282043326\n",
      "iteration 100 / 101: loss 0.46548382403597116\n",
      "iteration 0 / 101: loss 0.4641879747648845\n",
      "iteration 100 / 101: loss 0.4622726577817577\n",
      "iteration 0 / 101: loss 0.4631973218434563\n",
      "iteration 100 / 101: loss 0.4609656244107393\n",
      "iteration 0 / 101: loss 0.4617645986199197\n",
      "iteration 100 / 101: loss 0.46162372529833534\n",
      "iteration 0 / 101: loss 0.4664552634451733\n",
      "iteration 100 / 101: loss 0.461919595864546\n",
      "iteration 0 / 101: loss 0.45960428221744176\n",
      "iteration 100 / 101: loss 0.46631597124351587\n",
      "iteration 0 / 101: loss 0.46107041910244717\n",
      "iteration 100 / 101: loss 0.4628263295919709\n",
      "iteration 0 / 101: loss 0.46327260981435886\n",
      "iteration 100 / 101: loss 0.4589535917583734\n",
      "iteration 0 / 101: loss 0.463279353254185\n",
      "iteration 100 / 101: loss 0.4614439821875102\n",
      "iteration 0 / 101: loss 0.45818785202784473\n",
      "iteration 100 / 101: loss 0.45999965704851564\n",
      "iteration 0 / 101: loss 0.46142326990056304\n",
      "iteration 100 / 101: loss 0.46210492813607046\n",
      "iteration 0 / 101: loss 0.45951633081255566\n",
      "iteration 100 / 101: loss 0.46410088122152293\n",
      "iteration 0 / 101: loss 0.45893436409363086\n",
      "iteration 100 / 101: loss 0.46189161452165234\n",
      "iteration 0 / 101: loss 0.46478359876737835\n",
      "iteration 100 / 101: loss 0.4596845162728562\n",
      "iteration 0 / 101: loss 0.4650584889504431\n",
      "iteration 100 / 101: loss 0.46461816933096484\n",
      "iteration 0 / 101: loss 0.46015961800699134\n",
      "iteration 100 / 101: loss 0.46055726435193484\n",
      "iteration 0 / 101: loss 0.4606520615547781\n",
      "iteration 100 / 101: loss 0.46153030695093683\n",
      "iteration 0 / 101: loss 0.4627127805860675\n",
      "iteration 100 / 101: loss 0.4591200230697817\n",
      "iteration 0 / 101: loss 0.4645722255130657\n",
      "iteration 100 / 101: loss 0.45724856512469375\n",
      "iteration 0 / 101: loss 0.4633216348136225\n",
      "iteration 100 / 101: loss 0.45818634996535096\n",
      "iteration 0 / 101: loss 0.46506264156297117\n",
      "iteration 100 / 101: loss 0.4610191374903655\n",
      "iteration 0 / 101: loss 0.4616159324401317\n",
      "iteration 100 / 101: loss 0.4594469165403767\n",
      "iteration 0 / 101: loss 0.46256353909810405\n",
      "iteration 100 / 101: loss 0.46013103083425844\n",
      "iteration 0 / 101: loss 0.4647291747409284\n",
      "iteration 100 / 101: loss 0.4591713827341931\n",
      "iteration 0 / 101: loss 0.4603369325316659\n",
      "iteration 100 / 101: loss 0.4619214929431085\n",
      "iteration 0 / 101: loss 0.4561364261162407\n",
      "iteration 100 / 101: loss 0.4603767770528267\n",
      "iteration 0 / 101: loss 0.4639152827156757\n",
      "iteration 100 / 101: loss 0.4609957605923556\n",
      "iteration 0 / 101: loss 0.45671069733122527\n",
      "iteration 100 / 101: loss 0.4587662959465313\n",
      "iteration 0 / 101: loss 0.4598397281640955\n",
      "iteration 100 / 101: loss 0.4644427808492329\n",
      "iteration 0 / 101: loss 0.460982635082576\n",
      "iteration 100 / 101: loss 0.4594820601631126\n",
      "iteration 0 / 101: loss 0.4629813262389088\n",
      "iteration 100 / 101: loss 0.4552327176334572\n",
      "iteration 0 / 101: loss 0.4551691127123364\n",
      "iteration 100 / 101: loss 0.4569090545831072\n",
      "iteration 0 / 101: loss 0.46283016830911183\n",
      "iteration 100 / 101: loss 0.4543874497392651\n",
      "iteration 0 / 101: loss 0.45514248999973816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 101: loss 0.4587033721332882\n",
      "iteration 0 / 101: loss 0.45709313142022373\n",
      "iteration 100 / 101: loss 0.46280838753435544\n",
      "iteration 0 / 101: loss 0.45731087565116735\n",
      "iteration 100 / 101: loss 0.4594621688242081\n",
      "iteration 0 / 101: loss 0.46215327707752113\n",
      "iteration 100 / 101: loss 0.45920418595332063\n",
      "iteration 0 / 101: loss 0.4632027861248168\n",
      "iteration 100 / 101: loss 0.4598310613532578\n",
      "iteration 0 / 101: loss 0.45997080981951693\n",
      "iteration 100 / 101: loss 0.4595508877368617\n",
      "iteration 0 / 101: loss 0.45716395983350244\n",
      "iteration 100 / 101: loss 0.46022246024162367\n",
      "iteration 0 / 101: loss 0.4556724271736949\n",
      "iteration 100 / 101: loss 0.4575633886829785\n",
      "iteration 0 / 101: loss 0.4613401205412947\n",
      "iteration 100 / 101: loss 0.46162471838194896\n",
      "iteration 0 / 101: loss 0.4603290748938355\n",
      "iteration 100 / 101: loss 0.45427563523898096\n",
      "iteration 0 / 101: loss 0.45662883221778644\n",
      "iteration 100 / 101: loss 0.45697795975811695\n",
      "iteration 0 / 101: loss 0.4546214206359986\n",
      "iteration 100 / 101: loss 0.4531371877652316\n",
      "iteration 0 / 101: loss 0.45539358499850036\n",
      "iteration 100 / 101: loss 0.4523451832544999\n",
      "iteration 0 / 101: loss 0.4553890657339385\n",
      "iteration 100 / 101: loss 0.45903523880249725\n",
      "iteration 0 / 101: loss 0.46231451200282103\n",
      "iteration 100 / 101: loss 0.45974907365434536\n",
      "iteration 0 / 101: loss 0.4551690911989272\n",
      "iteration 100 / 101: loss 0.45480217393202466\n",
      "iteration 0 / 101: loss 0.4573661201318321\n",
      "iteration 100 / 101: loss 0.4536849944970414\n",
      "iteration 0 / 101: loss 0.4524321379410504\n",
      "iteration 100 / 101: loss 0.4602166901815408\n",
      "iteration 0 / 101: loss 0.4544248909300985\n",
      "iteration 100 / 101: loss 0.4561999085443275\n",
      "iteration 0 / 101: loss 0.4529596646175181\n",
      "iteration 100 / 101: loss 0.45457812697208927\n",
      "iteration 0 / 101: loss 0.4544923895636589\n",
      "iteration 100 / 101: loss 0.4597001490022374\n",
      "iteration 0 / 101: loss 0.4593030081483315\n",
      "iteration 100 / 101: loss 0.45878950591344514\n",
      "iteration 0 / 101: loss 0.45907530274291497\n",
      "iteration 100 / 101: loss 0.4514720146533959\n",
      "iteration 0 / 101: loss 0.46075114933818057\n",
      "iteration 100 / 101: loss 0.4557127621555661\n",
      "iteration 0 / 101: loss 0.45352284544433363\n",
      "iteration 100 / 101: loss 0.4540911394515516\n",
      "iteration 0 / 101: loss 0.4547571052753195\n",
      "iteration 100 / 101: loss 0.45614456161467065\n",
      "iteration 0 / 101: loss 0.45426782738372373\n",
      "iteration 100 / 101: loss 0.45603310178277634\n",
      "iteration 0 / 101: loss 0.45733142871444926\n",
      "iteration 100 / 101: loss 0.45804526760938397\n",
      "iteration 0 / 101: loss 0.4622894816635007\n",
      "iteration 100 / 101: loss 0.45619544324998984\n",
      "iteration 0 / 101: loss 0.45358004173633837\n",
      "iteration 100 / 101: loss 0.452205771021086\n",
      "iteration 0 / 101: loss 0.4550804954328216\n",
      "iteration 100 / 101: loss 0.45337173747963827\n",
      "iteration 0 / 101: loss 0.4572300266550356\n",
      "iteration 100 / 101: loss 0.45892970372340547\n",
      "iteration 0 / 101: loss 0.4577891156050346\n",
      "iteration 100 / 101: loss 0.4524267803757664\n",
      "iteration 0 / 101: loss 0.45771663922949574\n",
      "iteration 100 / 101: loss 0.44956096711204624\n",
      "iteration 0 / 101: loss 0.459247570529992\n",
      "iteration 100 / 101: loss 0.45459081070072466\n",
      "iteration 0 / 101: loss 0.45802229166552205\n",
      "iteration 100 / 101: loss 0.4568651482142175\n",
      "iteration 0 / 101: loss 0.4573507192873155\n",
      "iteration 100 / 101: loss 0.45724727728751036\n",
      "iteration 0 / 101: loss 0.45570311890596377\n",
      "iteration 100 / 101: loss 0.45461009162993293\n",
      "iteration 0 / 101: loss 0.4584016203821063\n",
      "iteration 100 / 101: loss 0.4538194613200656\n",
      "iteration 0 / 101: loss 0.45824229140991435\n",
      "iteration 100 / 101: loss 0.45437387461137835\n",
      "iteration 0 / 101: loss 0.4538131311594908\n",
      "iteration 100 / 101: loss 0.45781014944618253\n",
      "iteration 0 / 101: loss 0.45469123097680064\n",
      "iteration 100 / 101: loss 0.45649146862005496\n",
      "iteration 0 / 101: loss 0.4522735038838435\n",
      "iteration 100 / 101: loss 0.4516012015190093\n",
      "iteration 0 / 101: loss 0.4580645363329768\n",
      "iteration 100 / 101: loss 0.4582243194101563\n",
      "iteration 0 / 101: loss 0.4546491596454048\n",
      "iteration 100 / 101: loss 0.4565490212477314\n",
      "iteration 0 / 101: loss 0.4551418864879107\n",
      "iteration 100 / 101: loss 0.4525686581767803\n",
      "iteration 0 / 101: loss 0.4536589160159789\n",
      "iteration 100 / 101: loss 0.45261251127855145\n",
      "iteration 0 / 101: loss 0.45205945873206427\n",
      "iteration 100 / 101: loss 0.44979713449275033\n",
      "iteration 0 / 101: loss 0.4571273286422619\n",
      "iteration 100 / 101: loss 0.45900753810677064\n",
      "iteration 0 / 101: loss 0.45796864075401533\n",
      "iteration 100 / 101: loss 0.4554846959668853\n",
      "iteration 0 / 101: loss 0.4566038602954709\n",
      "iteration 100 / 101: loss 0.45452674717321195\n",
      "iteration 0 / 101: loss 0.45597401397807646\n",
      "iteration 100 / 101: loss 0.45500367497472916\n",
      "iteration 0 / 101: loss 0.4546524563483852\n",
      "iteration 100 / 101: loss 0.4564308255591351\n",
      "iteration 0 / 101: loss 0.45680102800871236\n",
      "iteration 100 / 101: loss 0.4590793139625604\n",
      "iteration 0 / 101: loss 0.4541073814966289\n",
      "iteration 100 / 101: loss 0.4535740816833939\n",
      "iteration 0 / 101: loss 0.45798863735639567\n",
      "iteration 100 / 101: loss 0.45451710079892166\n",
      "iteration 0 / 101: loss 0.4509570471502161\n",
      "iteration 100 / 101: loss 0.4555420540288458\n",
      "iteration 0 / 101: loss 0.4562439928158522\n",
      "iteration 100 / 101: loss 0.45698623599898597\n",
      "iteration 0 / 101: loss 0.4542604775104969\n",
      "iteration 100 / 101: loss 0.45204252286727936\n",
      "iteration 0 / 101: loss 0.45019680817402524\n",
      "iteration 100 / 101: loss 0.4560599972077788\n",
      "iteration 0 / 101: loss 0.4555681141295962\n",
      "iteration 100 / 101: loss 0.4544189424933607\n",
      "iteration 0 / 101: loss 0.45905278170264097\n",
      "iteration 100 / 101: loss 0.4539074035906762\n",
      "iteration 0 / 101: loss 0.45247815153743165\n",
      "iteration 100 / 101: loss 0.4534361405240518\n",
      "iteration 0 / 101: loss 0.4574458714223903\n",
      "iteration 100 / 101: loss 0.456880842421876\n",
      "iteration 0 / 101: loss 0.45571037751958776\n",
      "iteration 100 / 101: loss 0.45381253000205146\n",
      "iteration 0 / 101: loss 0.4547692653523255\n",
      "iteration 100 / 101: loss 0.45402519600265795\n",
      "iteration 0 / 101: loss 0.4509261751748083\n",
      "iteration 100 / 101: loss 0.45645624426398584\n",
      "iteration 0 / 101: loss 0.4592613641585942\n",
      "iteration 100 / 101: loss 0.45499201269753675\n",
      "iteration 0 / 101: loss 0.45801092928930687\n",
      "iteration 100 / 101: loss 0.45873383803197143\n",
      "iteration 0 / 101: loss 0.45267321246466496\n",
      "iteration 100 / 101: loss 0.4561450400256135\n",
      "iteration 0 / 101: loss 0.45213912285934366\n",
      "iteration 100 / 101: loss 0.45449313024139504\n",
      "iteration 0 / 101: loss 0.45825546132713507\n",
      "iteration 100 / 101: loss 0.4561711630278733\n",
      "iteration 0 / 101: loss 0.4565450423020827\n",
      "iteration 100 / 101: loss 0.4549004856078197\n",
      "iteration 0 / 101: loss 0.4567312578769193\n",
      "iteration 100 / 101: loss 0.4524370333129639\n",
      "iteration 0 / 101: loss 0.4557047062727774\n",
      "iteration 100 / 101: loss 0.45197106406101895\n",
      "iteration 0 / 101: loss 0.4546388370052904\n",
      "iteration 100 / 101: loss 0.45214207048023597\n",
      "iteration 0 / 101: loss 0.45949050083614534\n",
      "iteration 100 / 101: loss 0.4510564333407858\n",
      "iteration 0 / 101: loss 0.45754876562469377\n",
      "iteration 100 / 101: loss 0.45673668786200905\n",
      "iteration 0 / 101: loss 0.45395796707053865\n",
      "iteration 100 / 101: loss 0.4509727603710069\n",
      "iteration 0 / 101: loss 0.44960715934540907\n",
      "iteration 100 / 101: loss 0.45447315995967347\n",
      "iteration 0 / 101: loss 0.450190979061677\n",
      "iteration 100 / 101: loss 0.4514185444210931\n",
      "iteration 0 / 101: loss 0.45544948139619496\n",
      "iteration 100 / 101: loss 0.4540932328432734\n",
      "iteration 0 / 101: loss 0.4527706450131561\n",
      "iteration 100 / 101: loss 0.45566632522231626\n",
      "iteration 0 / 101: loss 0.45177027555869836\n",
      "iteration 100 / 101: loss 0.4547599864707575\n",
      "iteration 0 / 101: loss 0.45444052297676213\n",
      "iteration 100 / 101: loss 0.45519474138999133\n",
      "iteration 0 / 101: loss 0.45086270824921926\n",
      "iteration 100 / 101: loss 0.45161643266494816\n",
      "iteration 0 / 101: loss 0.45470315946161904\n",
      "iteration 100 / 101: loss 0.4582389896250092\n",
      "iteration 0 / 101: loss 0.4509780019289737\n",
      "iteration 100 / 101: loss 0.454905852146058\n",
      "iteration 0 / 101: loss 0.45149548385919563\n",
      "iteration 100 / 101: loss 0.45129796594530736\n",
      "iteration 0 / 101: loss 0.4554651321962629\n",
      "iteration 100 / 101: loss 0.4497432173790425\n",
      "iteration 0 / 101: loss 0.45228871351850064\n",
      "iteration 100 / 101: loss 0.456566430143968\n",
      "iteration 0 / 101: loss 0.45049096586385257\n",
      "iteration 100 / 101: loss 0.4556458464501476\n",
      "iteration 0 / 101: loss 0.4553113528714136\n",
      "iteration 100 / 101: loss 0.4537333897498316\n",
      "iteration 0 / 101: loss 0.4533649107986574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 101: loss 0.45241409561740975\n",
      "iteration 0 / 101: loss 0.45890185772554726\n",
      "iteration 100 / 101: loss 0.4507051162146229\n",
      "iteration 0 / 101: loss 0.4550489172140748\n",
      "iteration 100 / 101: loss 0.4570719843301263\n",
      "iteration 0 / 101: loss 0.4551361007138719\n",
      "iteration 100 / 101: loss 0.45492663056593907\n",
      "iteration 0 / 101: loss 0.45534675830757904\n",
      "iteration 100 / 101: loss 0.4535587072487529\n",
      "iteration 0 / 101: loss 0.451394207091863\n",
      "iteration 100 / 101: loss 0.45677021096124026\n",
      "iteration 0 / 101: loss 0.45271718799011545\n",
      "iteration 100 / 101: loss 0.44562192859680716\n",
      "iteration 0 / 101: loss 0.45159761098438383\n",
      "iteration 100 / 101: loss 0.4551763159048335\n",
      "iteration 0 / 101: loss 0.4544399608634355\n",
      "iteration 100 / 101: loss 0.45410216897629985\n",
      "iteration 0 / 101: loss 0.4504663857556425\n",
      "iteration 100 / 101: loss 0.4511806271973186\n",
      "iteration 0 / 101: loss 0.45073011673718116\n",
      "iteration 100 / 101: loss 0.4551191658505441\n",
      "iteration 0 / 101: loss 0.4546922925094224\n",
      "iteration 100 / 101: loss 0.4523903257796272\n",
      "iteration 0 / 101: loss 0.45208556049962684\n",
      "iteration 100 / 101: loss 0.45085650870149824\n",
      "iteration 0 / 101: loss 0.4538245576852269\n",
      "iteration 100 / 101: loss 0.44817459878869403\n",
      "iteration 0 / 101: loss 0.454251786085207\n",
      "iteration 100 / 101: loss 0.45319341722688067\n",
      "iteration 0 / 101: loss 0.44851272336208\n",
      "iteration 100 / 101: loss 0.4562510426531932\n",
      "iteration 0 / 101: loss 0.45085876105931044\n",
      "iteration 100 / 101: loss 0.45249991503015874\n",
      "iteration 0 / 101: loss 0.45750726408629294\n",
      "iteration 100 / 101: loss 0.45169652828441603\n",
      "iteration 0 / 101: loss 0.457681946815064\n",
      "iteration 100 / 101: loss 0.4516173362728314\n",
      "iteration 0 / 101: loss 0.45428650722395975\n",
      "iteration 100 / 101: loss 0.44927615564364054\n",
      "iteration 0 / 101: loss 0.45567418297721296\n",
      "iteration 100 / 101: loss 0.45185402785704276\n",
      "iteration 0 / 101: loss 0.45578662692682914\n",
      "iteration 100 / 101: loss 0.45780084400956905\n",
      "iteration 0 / 101: loss 0.4486070093951706\n",
      "iteration 100 / 101: loss 0.4467304561905134\n",
      "iteration 0 / 101: loss 0.4517144206350511\n",
      "iteration 100 / 101: loss 0.45539519739597245\n",
      "iteration 0 / 101: loss 0.44938504349908065\n",
      "iteration 100 / 101: loss 0.4545601327361003\n",
      "iteration 0 / 101: loss 0.45223323903687224\n",
      "iteration 100 / 101: loss 0.45171549647588227\n",
      "iteration 0 / 101: loss 0.4515021478572484\n",
      "iteration 100 / 101: loss 0.4458045013573193\n",
      "iteration 0 / 101: loss 0.45392411249211967\n",
      "iteration 100 / 101: loss 0.4517549172898249\n",
      "iteration 0 / 101: loss 0.4501316563184457\n",
      "iteration 100 / 101: loss 0.45150941723785326\n",
      "iteration 0 / 101: loss 0.4497912860680947\n",
      "iteration 100 / 101: loss 0.4506266550773747\n",
      "iteration 0 / 101: loss 0.45025381510990614\n",
      "iteration 100 / 101: loss 0.447461192170232\n",
      "iteration 0 / 101: loss 0.4516667477588507\n",
      "iteration 100 / 101: loss 0.45462423653812717\n",
      "iteration 0 / 101: loss 0.4479113292461451\n",
      "iteration 100 / 101: loss 0.4508932625556723\n",
      "iteration 0 / 101: loss 0.4524437076773007\n",
      "iteration 100 / 101: loss 0.4495232865870473\n",
      "iteration 0 / 101: loss 0.4548750213560113\n",
      "iteration 100 / 101: loss 0.45158237742914936\n",
      "iteration 0 / 101: loss 0.45153672211534357\n",
      "iteration 100 / 101: loss 0.45426343810210157\n",
      "iteration 0 / 101: loss 0.45086905605673383\n",
      "iteration 100 / 101: loss 0.44531154960803276\n",
      "iteration 0 / 101: loss 0.4551346765818686\n",
      "iteration 100 / 101: loss 0.45663131439163956\n",
      "iteration 0 / 101: loss 0.4524633326959097\n",
      "iteration 100 / 101: loss 0.45397331792948575\n",
      "iteration 0 / 101: loss 0.45251248417073225\n",
      "iteration 100 / 101: loss 0.4505934482840167\n",
      "iteration 0 / 101: loss 0.4528311451203843\n",
      "iteration 100 / 101: loss 0.4473927860625575\n",
      "iteration 0 / 101: loss 0.44931720542281955\n",
      "iteration 100 / 101: loss 0.4542366377179887\n",
      "iteration 0 / 101: loss 0.4516605584150409\n",
      "iteration 100 / 101: loss 0.4522516012883844\n",
      "iteration 0 / 101: loss 0.45117602770748666\n",
      "iteration 100 / 101: loss 0.45503204187897844\n",
      "iteration 0 / 101: loss 0.45295048043672054\n",
      "iteration 100 / 101: loss 0.4528329799380689\n",
      "iteration 0 / 101: loss 0.4479082803346596\n",
      "iteration 100 / 101: loss 0.4475231074104751\n",
      "iteration 0 / 101: loss 0.4550925548341934\n",
      "iteration 100 / 101: loss 0.4472398702350044\n",
      "iteration 0 / 101: loss 0.4533017528232387\n",
      "iteration 100 / 101: loss 0.4521633542572662\n",
      "iteration 0 / 101: loss 0.45292842906865327\n",
      "iteration 100 / 101: loss 0.45003251956272416\n",
      "iteration 0 / 101: loss 0.4526893418764605\n",
      "iteration 100 / 101: loss 0.45614032226798795\n",
      "iteration 0 / 101: loss 0.45294882981633783\n",
      "iteration 100 / 101: loss 0.4536068435827542\n",
      "iteration 0 / 101: loss 0.45321559523255084\n",
      "iteration 100 / 101: loss 0.4483718653763365\n",
      "iteration 0 / 101: loss 0.4526914674140191\n",
      "iteration 100 / 101: loss 0.4578968296226557\n",
      "iteration 0 / 101: loss 0.45027240714884503\n",
      "iteration 100 / 101: loss 0.4458812992826623\n",
      "iteration 0 / 101: loss 0.4543733365343366\n",
      "iteration 100 / 101: loss 0.4533176021411863\n",
      "iteration 0 / 101: loss 0.4504356516274378\n",
      "iteration 100 / 101: loss 0.44807031906129735\n",
      "iteration 0 / 101: loss 0.44711898823150353\n",
      "iteration 100 / 101: loss 0.449555852881826\n",
      "iteration 0 / 101: loss 0.4503568994370624\n",
      "iteration 100 / 101: loss 0.45538439831655075\n",
      "iteration 0 / 101: loss 0.4521968629927289\n",
      "iteration 100 / 101: loss 0.4514504483497688\n",
      "iteration 0 / 101: loss 0.4500486625114158\n",
      "iteration 100 / 101: loss 0.4519657089016854\n",
      "iteration 0 / 101: loss 0.4524611406701643\n",
      "iteration 100 / 101: loss 0.44944182875779926\n",
      "iteration 0 / 101: loss 0.4532285966454686\n",
      "iteration 100 / 101: loss 0.45200889767650015\n",
      "iteration 0 / 101: loss 0.44708741131801044\n",
      "iteration 100 / 101: loss 0.45076141699528893\n",
      "iteration 0 / 101: loss 0.4531793629458156\n",
      "iteration 100 / 101: loss 0.45316871483943105\n",
      "iteration 0 / 101: loss 0.4512923005422373\n",
      "iteration 100 / 101: loss 0.45008991080376715\n",
      "iteration 0 / 101: loss 0.4481510787359046\n",
      "iteration 100 / 101: loss 0.45245357537731534\n",
      "iteration 0 / 101: loss 0.44900116347340374\n",
      "iteration 100 / 101: loss 0.4510382361431868\n",
      "iteration 0 / 101: loss 0.450272734144425\n",
      "iteration 100 / 101: loss 0.44841161740689267\n",
      "iteration 0 / 101: loss 0.45331007125132305\n",
      "iteration 100 / 101: loss 0.4499429964547697\n",
      "iteration 0 / 101: loss 0.4525324676129449\n",
      "iteration 100 / 101: loss 0.4519882938401971\n",
      "iteration 0 / 101: loss 0.45005779222035175\n",
      "iteration 100 / 101: loss 0.4519054500046147\n",
      "iteration 0 / 101: loss 0.4484099656877447\n",
      "iteration 100 / 101: loss 0.45285040387547215\n",
      "iteration 0 / 101: loss 0.4537422494871779\n",
      "iteration 100 / 101: loss 0.4529111427470749\n",
      "iteration 0 / 101: loss 0.4523850820912501\n",
      "iteration 100 / 101: loss 0.45334459353583634\n",
      "iteration 0 / 101: loss 0.450229686798984\n",
      "iteration 100 / 101: loss 0.44911622787798244\n",
      "iteration 0 / 101: loss 0.44787588823204266\n",
      "iteration 100 / 101: loss 0.4492998389197712\n",
      "iteration 0 / 101: loss 0.443499617910606\n",
      "iteration 100 / 101: loss 0.450814707262684\n",
      "iteration 0 / 101: loss 0.4465754139527336\n",
      "iteration 100 / 101: loss 0.44989060734503666\n",
      "iteration 0 / 101: loss 0.45019464359307015\n",
      "iteration 100 / 101: loss 0.4513429771701783\n",
      "iteration 0 / 101: loss 0.4537560095430018\n",
      "iteration 100 / 101: loss 0.4502147531041693\n",
      "iteration 0 / 101: loss 0.44887632925098814\n",
      "iteration 100 / 101: loss 0.450303828121601\n",
      "iteration 0 / 101: loss 0.4530988441335013\n",
      "iteration 100 / 101: loss 0.44719095125000585\n",
      "iteration 0 / 101: loss 0.4511925695410126\n",
      "iteration 100 / 101: loss 0.4486178816350344\n",
      "iteration 0 / 101: loss 0.45072411205133317\n",
      "iteration 100 / 101: loss 0.44917714972537626\n",
      "iteration 0 / 101: loss 0.4521068153343889\n",
      "iteration 100 / 101: loss 0.4469708602419021\n",
      "iteration 0 / 101: loss 0.44661169922138133\n",
      "iteration 100 / 101: loss 0.44724166734725185\n",
      "iteration 0 / 101: loss 0.45286969918883097\n",
      "iteration 100 / 101: loss 0.4519225154492196\n",
      "iteration 0 / 101: loss 0.44939175821327176\n",
      "iteration 100 / 101: loss 0.4528615592517673\n",
      "iteration 0 / 101: loss 0.45434082402425513\n",
      "iteration 100 / 101: loss 0.4520652705109592\n",
      "iteration 0 / 101: loss 0.45093997667587865\n",
      "iteration 100 / 101: loss 0.44419300986348187\n",
      "iteration 0 / 101: loss 0.4480938002672583\n",
      "iteration 100 / 101: loss 0.4519774118047721\n",
      "iteration 0 / 101: loss 0.4519376018582093\n",
      "iteration 100 / 101: loss 0.4479618085936058\n",
      "iteration 0 / 101: loss 0.4460065796322173\n",
      "iteration 100 / 101: loss 0.4483477191579355\n",
      "iteration 0 / 101: loss 0.44547540878811204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 101: loss 0.4477318395570099\n",
      "iteration 0 / 101: loss 0.44937302114211897\n",
      "iteration 100 / 101: loss 0.4515160393330551\n",
      "iteration 0 / 101: loss 0.4494362523339294\n",
      "iteration 100 / 101: loss 0.4508092952276284\n",
      "iteration 0 / 101: loss 0.4478457288499193\n",
      "iteration 100 / 101: loss 0.45130714463696125\n",
      "iteration 0 / 101: loss 0.4564297488162087\n",
      "iteration 100 / 101: loss 0.44838723824987675\n",
      "iteration 0 / 101: loss 0.453821795761587\n",
      "iteration 100 / 101: loss 0.4503877813604011\n",
      "iteration 0 / 101: loss 0.4510817080676799\n",
      "iteration 100 / 101: loss 0.447874383498509\n",
      "iteration 0 / 101: loss 0.44885911150542707\n",
      "iteration 100 / 101: loss 0.4456225581077009\n",
      "iteration 0 / 101: loss 0.44749862938529605\n",
      "iteration 100 / 101: loss 0.44996011071111075\n",
      "iteration 0 / 101: loss 0.452737357154365\n",
      "iteration 100 / 101: loss 0.4499356140477553\n",
      "iteration 0 / 101: loss 0.4442085356031172\n",
      "iteration 100 / 101: loss 0.44948712624713755\n",
      "iteration 0 / 101: loss 0.4518303243052675\n",
      "iteration 100 / 101: loss 0.45207757995481934\n",
      "iteration 0 / 101: loss 0.44227075488604217\n",
      "iteration 100 / 101: loss 0.4513105891982812\n",
      "iteration 0 / 101: loss 0.45035654906708894\n",
      "iteration 100 / 101: loss 0.45004879646217544\n",
      "iteration 0 / 101: loss 0.4486787096958125\n",
      "iteration 100 / 101: loss 0.4445155775568281\n",
      "iteration 0 / 101: loss 0.4476510251443341\n",
      "iteration 100 / 101: loss 0.44742477862073415\n",
      "iteration 0 / 101: loss 0.44688441486019453\n",
      "iteration 100 / 101: loss 0.447632986196762\n",
      "iteration 0 / 101: loss 0.4442319298433232\n",
      "iteration 100 / 101: loss 0.44817966629810907\n",
      "iteration 0 / 101: loss 0.4499685766789315\n",
      "iteration 100 / 101: loss 0.44939545923412766\n",
      "iteration 0 / 101: loss 0.44679225505946096\n",
      "iteration 100 / 101: loss 0.44761787531841446\n",
      "iteration 0 / 101: loss 0.44704697817256656\n",
      "iteration 100 / 101: loss 0.4498548735307249\n",
      "iteration 0 / 101: loss 0.4462870255861955\n",
      "iteration 100 / 101: loss 0.44851278798461125\n",
      "iteration 0 / 101: loss 0.44597063715298174\n",
      "iteration 100 / 101: loss 0.44709137627028483\n",
      "iteration 0 / 101: loss 0.4499542358921536\n",
      "iteration 100 / 101: loss 0.45251006219606116\n",
      "iteration 0 / 101: loss 0.44811088770716606\n",
      "iteration 100 / 101: loss 0.4489663809914851\n",
      "iteration 0 / 101: loss 0.4488188257405262\n",
      "iteration 100 / 101: loss 0.4449815376111498\n",
      "iteration 0 / 101: loss 0.4468511644494815\n",
      "iteration 100 / 101: loss 0.44814336894959306\n",
      "iteration 0 / 101: loss 0.44459031111229885\n",
      "iteration 100 / 101: loss 0.44952689854866584\n",
      "iteration 0 / 101: loss 0.4450338847736084\n",
      "iteration 100 / 101: loss 0.44457983593603245\n",
      "iteration 0 / 101: loss 0.44928940511312404\n",
      "iteration 100 / 101: loss 0.4464800413002878\n",
      "iteration 0 / 101: loss 0.44896244749812586\n",
      "iteration 100 / 101: loss 0.44840307243652683\n",
      "iteration 0 / 101: loss 0.4449424437287847\n",
      "iteration 100 / 101: loss 0.44886882794619204\n",
      "iteration 0 / 101: loss 0.4449919366015511\n",
      "iteration 100 / 101: loss 0.44406187008227455\n",
      "iteration 0 / 101: loss 0.4463523991578129\n",
      "iteration 100 / 101: loss 0.44590363114247905\n",
      "iteration 0 / 101: loss 0.4475100696722351\n",
      "iteration 100 / 101: loss 0.44446357942373715\n",
      "iteration 0 / 101: loss 0.45210861237329275\n",
      "iteration 100 / 101: loss 0.4498757974973392\n",
      "iteration 0 / 101: loss 0.4452974448161322\n",
      "iteration 100 / 101: loss 0.44704091173552235\n",
      "iteration 0 / 101: loss 0.4481085632143795\n",
      "iteration 100 / 101: loss 0.44580735847399505\n",
      "iteration 0 / 101: loss 0.4460914834619949\n",
      "iteration 100 / 101: loss 0.4435428826857704\n",
      "iteration 0 / 101: loss 0.44396588750824284\n",
      "iteration 100 / 101: loss 0.4422893276045626\n",
      "iteration 0 / 101: loss 0.448214240824944\n",
      "iteration 100 / 101: loss 0.443834226504\n",
      "iteration 0 / 101: loss 0.4495251121390515\n",
      "iteration 100 / 101: loss 0.4494021877800383\n",
      "iteration 0 / 101: loss 0.44616930818285266\n",
      "iteration 100 / 101: loss 0.4471883612949169\n",
      "iteration 0 / 101: loss 0.4458635569294932\n",
      "iteration 100 / 101: loss 0.44699874535711537\n",
      "iteration 0 / 101: loss 0.4482225102302562\n",
      "iteration 100 / 101: loss 0.4501345598751532\n",
      "iteration 0 / 101: loss 0.44978133943716897\n",
      "iteration 100 / 101: loss 0.44755413908897185\n",
      "iteration 0 / 101: loss 0.44526393305366074\n",
      "iteration 100 / 101: loss 0.4476704869681005\n",
      "iteration 0 / 101: loss 0.4469613652210733\n",
      "iteration 100 / 101: loss 0.4476037352522052\n",
      "iteration 0 / 101: loss 0.44195647973963487\n",
      "iteration 100 / 101: loss 0.44311967790832174\n",
      "iteration 0 / 101: loss 0.44746675564564165\n",
      "iteration 100 / 101: loss 0.4465880829860412\n",
      "iteration 0 / 101: loss 0.4466582485453232\n",
      "iteration 100 / 101: loss 0.44896180009582465\n",
      "iteration 0 / 101: loss 0.44689287219609053\n",
      "iteration 100 / 101: loss 0.45016910623609413\n",
      "iteration 0 / 101: loss 0.4496451372415725\n",
      "iteration 100 / 101: loss 0.44559965337307844\n",
      "iteration 0 / 101: loss 0.4517973935727855\n",
      "iteration 100 / 101: loss 0.4492005936528491\n",
      "iteration 0 / 101: loss 0.44570706550871836\n",
      "iteration 100 / 101: loss 0.44989107242353077\n",
      "iteration 0 / 101: loss 0.44542064395277436\n",
      "iteration 100 / 101: loss 0.44329192930964717\n",
      "iteration 0 / 101: loss 0.4459301507771922\n",
      "iteration 100 / 101: loss 0.447126972856128\n",
      "iteration 0 / 101: loss 0.44587760595256903\n",
      "iteration 100 / 101: loss 0.44692666427503336\n",
      "iteration 0 / 101: loss 0.44763426026195274\n",
      "iteration 100 / 101: loss 0.44661928435414483\n",
      "iteration 0 / 101: loss 0.4484944532121058\n",
      "iteration 100 / 101: loss 0.4493762065506572\n",
      "iteration 0 / 101: loss 0.44361283061510154\n",
      "iteration 100 / 101: loss 0.44595183927852555\n",
      "iteration 0 / 101: loss 0.4407787670331173\n",
      "iteration 100 / 101: loss 0.4441565324301598\n",
      "iteration 0 / 101: loss 0.4478332789787387\n",
      "iteration 100 / 101: loss 0.445196123714145\n",
      "iteration 0 / 101: loss 0.4460137185028138\n",
      "iteration 100 / 101: loss 0.44714578261557847\n",
      "iteration 0 / 101: loss 0.4484874402900777\n",
      "iteration 100 / 101: loss 0.44699600118235966\n",
      "iteration 0 / 101: loss 0.4515285017061769\n",
      "iteration 100 / 101: loss 0.43957683331749275\n",
      "iteration 0 / 101: loss 0.4473389960393125\n",
      "iteration 100 / 101: loss 0.44357588832835265\n",
      "iteration 0 / 101: loss 0.44745250085989097\n",
      "iteration 100 / 101: loss 0.45117805573666026\n",
      "iteration 0 / 101: loss 0.44635011178065603\n",
      "iteration 100 / 101: loss 0.4468752228801014\n",
      "iteration 0 / 101: loss 0.4389513095352212\n",
      "iteration 100 / 101: loss 0.44715570217803013\n",
      "iteration 0 / 101: loss 0.4512932847770692\n",
      "iteration 100 / 101: loss 0.4481119502501284\n",
      "iteration 0 / 101: loss 0.44870125476977285\n",
      "iteration 100 / 101: loss 0.44186378548182625\n",
      "iteration 0 / 101: loss 0.44937735660469635\n",
      "iteration 100 / 101: loss 0.4455715615860457\n",
      "iteration 0 / 101: loss 0.44475492087667196\n",
      "iteration 100 / 101: loss 0.4473518927563859\n",
      "iteration 0 / 101: loss 0.4478259346617861\n",
      "iteration 100 / 101: loss 0.4428363750807684\n",
      "iteration 0 / 101: loss 0.44231579857182235\n",
      "iteration 100 / 101: loss 0.44634149472880685\n",
      "iteration 0 / 101: loss 0.4408545844982064\n",
      "iteration 100 / 101: loss 0.44815166287308855\n",
      "iteration 0 / 101: loss 0.4451046173231217\n",
      "iteration 100 / 101: loss 0.44290768404788305\n",
      "iteration 0 / 101: loss 0.44124933706684927\n",
      "iteration 100 / 101: loss 0.4478678879039921\n",
      "iteration 0 / 101: loss 0.44853201264452264\n",
      "iteration 100 / 101: loss 0.44439163072754306\n",
      "iteration 0 / 101: loss 0.44691376563352914\n",
      "iteration 100 / 101: loss 0.4427466564068526\n",
      "iteration 0 / 101: loss 0.4444545586330087\n",
      "iteration 100 / 101: loss 0.44893451950346114\n",
      "iteration 0 / 101: loss 0.449225420656263\n",
      "iteration 100 / 101: loss 0.44393849510635774\n",
      "iteration 0 / 101: loss 0.44303956708175524\n",
      "iteration 100 / 101: loss 0.4465793743115912\n",
      "iteration 0 / 101: loss 0.4487874080266744\n",
      "iteration 100 / 101: loss 0.44454365853036293\n",
      "iteration 0 / 101: loss 0.44778224666167005\n",
      "iteration 100 / 101: loss 0.44727020551775737\n",
      "iteration 0 / 101: loss 0.4511437640757906\n",
      "iteration 100 / 101: loss 0.4422067803734664\n",
      "iteration 0 / 101: loss 0.4412421262735536\n",
      "iteration 100 / 101: loss 0.4427208976857815\n",
      "iteration 0 / 101: loss 0.44074932353225843\n",
      "iteration 100 / 101: loss 0.44365187943736845\n",
      "iteration 0 / 101: loss 0.44225660993614585\n",
      "iteration 100 / 101: loss 0.45063026836339365\n",
      "iteration 0 / 101: loss 0.4444987560672896\n",
      "iteration 100 / 101: loss 0.4409588856310686\n",
      "iteration 0 / 101: loss 0.44600779079716507\n",
      "iteration 100 / 101: loss 0.435432836892812\n",
      "iteration 0 / 101: loss 0.44376518854167146\n",
      "iteration 100 / 101: loss 0.44708291282819784\n",
      "iteration 0 / 101: loss 0.44464512092928193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 101: loss 0.44567198759566773\n",
      "iteration 0 / 101: loss 0.44503370988882857\n",
      "iteration 100 / 101: loss 0.4442835294638655\n",
      "iteration 0 / 101: loss 0.4426787894408614\n",
      "iteration 100 / 101: loss 0.44772249826307725\n",
      "iteration 0 / 101: loss 0.44158526439633566\n",
      "iteration 100 / 101: loss 0.4458567205374778\n",
      "iteration 0 / 101: loss 0.44524065275846214\n",
      "iteration 100 / 101: loss 0.44718470268924576\n",
      "iteration 0 / 101: loss 0.44153717579861435\n",
      "iteration 100 / 101: loss 0.44456914557601196\n",
      "iteration 0 / 101: loss 0.4439070950309921\n",
      "iteration 100 / 101: loss 0.44416720467820997\n",
      "iteration 0 / 101: loss 0.44401876626664255\n",
      "iteration 100 / 101: loss 0.43870223708735606\n",
      "iteration 0 / 101: loss 0.4419223966087683\n",
      "iteration 100 / 101: loss 0.4400689359276562\n",
      "iteration 0 / 101: loss 0.44457889513382765\n",
      "iteration 100 / 101: loss 0.44209474514906355\n",
      "iteration 0 / 101: loss 0.44588034371388463\n",
      "iteration 100 / 101: loss 0.44195645644514664\n",
      "iteration 0 / 101: loss 0.44799719423440765\n",
      "iteration 100 / 101: loss 0.44246871335695026\n",
      "iteration 0 / 101: loss 0.44359814900178207\n",
      "iteration 100 / 101: loss 0.4473611528178895\n",
      "iteration 0 / 101: loss 0.44199592431083773\n",
      "iteration 100 / 101: loss 0.4399854349448455\n",
      "iteration 0 / 101: loss 0.4402373904210459\n",
      "iteration 100 / 101: loss 0.44738411320926375\n",
      "iteration 0 / 101: loss 0.4450851610768389\n",
      "iteration 100 / 101: loss 0.4436774457001901\n",
      "iteration 0 / 101: loss 0.44772224579993997\n",
      "iteration 100 / 101: loss 0.44353748724923453\n",
      "iteration 0 / 101: loss 0.44647442290898\n",
      "iteration 100 / 101: loss 0.4395017093385851\n",
      "iteration 0 / 101: loss 0.4427536319686149\n",
      "iteration 100 / 101: loss 0.44104080533833867\n",
      "iteration 0 / 101: loss 0.44950157166586907\n",
      "iteration 100 / 101: loss 0.4432522808671863\n",
      "iteration 0 / 101: loss 0.4462033640507958\n",
      "iteration 100 / 101: loss 0.4391519356384811\n",
      "iteration 0 / 101: loss 0.44630942458314093\n",
      "iteration 100 / 101: loss 0.4431789835219306\n",
      "iteration 0 / 101: loss 0.44150220254625316\n",
      "iteration 100 / 101: loss 0.44486938285329036\n",
      "iteration 0 / 101: loss 0.4436710503993722\n",
      "iteration 100 / 101: loss 0.4438600113368873\n",
      "iteration 0 / 101: loss 0.44107308896095776\n",
      "iteration 100 / 101: loss 0.4460127600774314\n",
      "iteration 0 / 101: loss 0.4405200084106322\n",
      "iteration 100 / 101: loss 0.4392182437226487\n",
      "iteration 0 / 101: loss 0.44551750452813993\n",
      "iteration 100 / 101: loss 0.4430941638905974\n",
      "iteration 0 / 101: loss 0.4386991836773853\n",
      "iteration 100 / 101: loss 0.4448408179055722\n",
      "iteration 0 / 101: loss 0.4482494983407292\n",
      "iteration 100 / 101: loss 0.44193003582502993\n",
      "iteration 0 / 101: loss 0.4432667485734222\n",
      "iteration 100 / 101: loss 0.4413401566101169\n",
      "iteration 0 / 101: loss 0.4419163145565232\n",
      "iteration 100 / 101: loss 0.4418564556079255\n",
      "iteration 0 / 101: loss 0.4430369689217106\n",
      "iteration 100 / 101: loss 0.4431531551126661\n",
      "iteration 0 / 101: loss 0.446068746723372\n",
      "iteration 100 / 101: loss 0.4481826018299652\n",
      "iteration 0 / 101: loss 0.44392046929007034\n",
      "iteration 100 / 101: loss 0.4401334273145152\n",
      "iteration 0 / 101: loss 0.43835890477422546\n",
      "iteration 100 / 101: loss 0.44164656297460353\n",
      "iteration 0 / 101: loss 0.4394698950543745\n",
      "iteration 100 / 101: loss 0.4493101290342695\n",
      "iteration 0 / 101: loss 0.44810561361733053\n",
      "iteration 100 / 101: loss 0.44389231860701905\n",
      "iteration 0 / 101: loss 0.44022703746001524\n"
     ]
    }
   ],
   "source": [
    "best_net = None # store the best model into this \n",
    "\n",
    "# ================================================================ #\n",
    "# YOUR CODE HERE:\n",
    "#   Optimize over your hyperparameters to arrive at the best neural\n",
    "#   network.  You should be able to get over 45% validation accuracy.\n",
    "#   For this part of the notebook, we will give credit based on the\n",
    "#   accuracy you get.  Your score on this question will be multiplied by:\n",
    "#      min(floor((X - 23%)) / %22, 1) \n",
    "#   where if you get 50% or higher validation accuracy, you get full\n",
    "#   points.\n",
    "#\n",
    "#   Note, you need to use the same network structure (keep hidden_size = 50)!\n",
    "# ================================================================ #\n",
    "\n",
    "# todo: optimal parameter search (you can use grid search by for-loops )\n",
    "\n",
    "input_size = 32 * 32 * 3 # do not change\n",
    "hidden_size = 50 # do not change\n",
    "num_classes = 10 # do not change\n",
    "best_valacc = 0 # do not change\n",
    "\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "best_acc = 0\n",
    "# Train the network and find best parameter: \n",
    "for batch in range(1,500, 50):\n",
    "    for learning in range(1, 10, 1):\n",
    "        for decay in range(50, 100, 5):\n",
    "            stats = net.train(X_train, y_train, X_val, y_val,\n",
    "                              num_iters=1000, batch_size=batch,\n",
    "                              learning_rate=(learning/10000000),\n",
    "                              learning_rate_decay=(decay/100),\n",
    "                              reg=0.1, verbose=True)\n",
    "                # Predict on the validation set\n",
    "                val_acc = (net.predict(X_val) == y_val).mean()\n",
    "                if val_acc > best_acc:\n",
    "                    best_acc = val_acc\n",
    "                    best_net = net\n",
    "\n",
    "best_valacc = best_acc\n",
    "# Output your results\n",
    "print(\"== Best parameter settings ==\")\n",
    "# print your best parameter setting here!\n",
    "print(\"Best accuracy on validation set: {}\".format(best_valacc))\n",
    "# ================================================================ #\n",
    "# END YOUR CODE HERE\n",
    "# ================================================================ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quesions\n",
    "\n",
    "(1) What is your best parameter settings? (Output from the previous cell)\n",
    "\n",
    "(2) What parameters did you tune? How are they changing the performance of nerural network? You can discuss any observations from the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the weights of your neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs145.vis_utils import visualize_grid\n",
    "\n",
    "# Visualize the weights of the network\n",
    "\n",
    "def show_net_weights(net):\n",
    "    W1 = net.params['W1']\n",
    "    W1 = W1.T.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)\n",
    "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_net_weights(subopt_net)\n",
    "show_net_weights(best_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "What differences do you see in the weights between the suboptimal net and the best net you arrived at? What do the weights in neural networks probably learn after training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = (best_net.predict(X_test) == y_test).mean()\n",
    "#test_acc = (subopt_net.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "(1) What is your test accuracy by using the best NN you have got? How much does the performance increase compared with kNN? Why can neural networks perform better than kNN?\n",
    "\n",
    "(2) Do you have any other ideas or suggestions to further improve the performance of neural networks other than the parameters you have tried in the homework? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question: Change MSE Loss to Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bonus question. If you finish this (cross entropy loss) correctly, you will get **up to 20 points** (add up to your HW3 score). \n",
    "\n",
    "Note: From grading policy of this course, your maximum points from homework are still 25 out of 100, but you can use the bonus question to make up other deduction of other assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass output scores in networks from forward pass into softmax function. The softmax function is defined as,\n",
    "$$p_j = \\sigma(z_j) = \\frac{e^{z_j}}{\\sum_{c=1}^{C} e^{z_c}}$$\n",
    "After softmax, the scores can be considered as probability of $j$-th class.\n",
    "\n",
    "The cross entropy loss is defined as,\n",
    "$$L = L_{\\text{CE}}+L_{reg} = \\frac{1}{N}\\sum_{i=1}^{N} \\log \\left(p_{i,j}\\right)+ \\frac{\\lambda}{2} \\left(||W_1||^2 + ||W_2||^2 \\right)$$\n",
    "\n",
    "To take derivative of this loss, you will get the gradient as,\n",
    "$$\\frac{\\partial L_{\\text{CE}}}{\\partial o_i} = p_i - y_i $$\n",
    "\n",
    "More details about multi-class cross entropy loss, please check [http://cs231n.github.io/linear-classify/](http://cs231n.github.io/linear-classify/) and [more explanation](https://deepnotes.io/softmax-crossentropy) about the derivative of cross entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the loss from MSE to cross entropy, you only need to change you ```MSE_loss(x,y)``` in ```TwoLayerNet.loss()``` function to ```softmax_loss(x,y)```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now you are free to use any code to show your results of the two-layer networks with newly-implemented cross entropy loss. You can use code from previous cells.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training your networks and show your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
